{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "from annoy import AnnoyIndex\n",
    "from bisect import bisect_left\n",
    "import re\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import combinations\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from union_find import *\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- Load DF without Clusters\n",
    "- Load Embeddings\n",
    "- Add Columns for clusters\n",
    "\n",
    "- Number of Fact-checks; Singleton Ratio over time \n",
    "- Consistency Measures Mode & Entropy by Verdict\n",
    "- Percentage of Clusters by Threshhold by number of languages\n",
    "- Actual vs Random Percentage by number of Languages\n",
    "- Mean Intra-Cluster Distance \n",
    "- Average Inter-Cluster Distance\n",
    "- Density of most dissimilar claims\n",
    "- Cosine Similarity of most dissimilar claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DF Without Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"../Data/minimal_FactCheckData_local.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "# Assure that datePublished is a datetime object\n",
    "df[\"datePublished\"] = pd.to_datetime(df.datePublished, errors=\"coerce\")\n",
    "print(f\"Number of NaNs in datePublished: {df.datePublished.isna().sum()}\")\n",
    "\n",
    "df = df.dropna(subset=[\"datePublished\"])\n",
    "# Save the dates in dictionary for quick lookup in cluster mapping\n",
    "dates = df.set_index(\"claim_minimal\").datePublished.to_dict()\n",
    "\n",
    "# Create a List of IDs for the cluster mapping\n",
    "ORIGINAL_IDS = df.claim_minimal.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create cluster mapping\n",
    "def create_edgelist(msd, threshold):\n",
    "    \"\"\"\n",
    "    Create edgelist from dictionary.\n",
    "    \"\"\"\n",
    "    edge_list = []\n",
    "    for key in msd.keys():\n",
    "        for edge in msd[key]:\n",
    "            if edge[1] >= threshold and dates[edge[0]] <= dates[key]:\n",
    "                edge_list.append((key, edge[0]))\n",
    "    return edge_list\n",
    "\n",
    "\n",
    "# Retain dictionary structure but remove edges below threshold\n",
    "def create_dict(msd, threshold):\n",
    "    \"\"\"\n",
    "    Remove edges below threshold.\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for key in msd.keys():\n",
    "        new_dict[key] = []\n",
    "        for edge in msd[key]:\n",
    "            if edge[1] >= threshold:\n",
    "                new_dict[key].append(edge)\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for quick lookup of id -> domain\n",
    "domains = df.set_index(\"claim_minimal\")[\"domain\"]\n",
    "\n",
    "# Load Edge List\n",
    "with open(\"../Data/edge_list.pkl\", \"rb\") as f:\n",
    "    msd = pickle.load(f)\n",
    "\n",
    "# Change to list of tuples\n",
    "msd = {k: list(v.items()) for k, v in msd.items()}\n",
    "\n",
    "# Prune the dictionary with minimal similarity of 0.75\n",
    "msd_small = create_dict(msd, 0.75)\n",
    "\n",
    "# Generate list of all keys, Initialize list of removed keys, shuffle keys\n",
    "all_keys = list(msd_small.keys())\n",
    "removed_keys = []\n",
    "np.random.shuffle(all_keys)\n",
    "\n",
    "# 1) Scramble the keys to assure that everything is random.\n",
    "# 2) Iterate through the keys. Remove a key iff any value is >0.99 (Except i==j)\n",
    "#    2.5) Iterate through all other keys and remove the key from the values.\n",
    "# 3) Repeat 2) until no more keys are removed.\n",
    "for ann in tqdm(all_keys, desc=\"Removing Entries with high similarity and same domain\"):\n",
    "    if ann in removed_keys:\n",
    "        raise Exception(\"This should not happen\")\n",
    "\n",
    "    for i in range(len(msd_small[ann])):\n",
    "        if msd_small[ann][i][1] < 0.95:\n",
    "            # Within each key the values are sorted by distance. So if one is < 0,95 all the rest will be too.\n",
    "            break\n",
    "        # If its not the same key and the domains are the same\n",
    "        if (\n",
    "            ann != msd_small[ann][i][0]\n",
    "            and domains[ann] == domains[msd_small[ann][i][0]]\n",
    "        ):\n",
    "            msd_small.pop(ann)\n",
    "            removed_keys.append(ann)\n",
    "            # We remove the remove KEY from all VALUES.\n",
    "            for key in msd_small.keys():\n",
    "                if ann in msd_small[key]:\n",
    "                    msd_small[key].remove(ann)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list of thresholds to investigate\n",
    "thresholds = [0.75, 0.775, 0.8, 0.825, 0.85, 0.875, 0.9, 0.925, 0.95]\n",
    "clusters = {}\n",
    "edge_lists = {}\n",
    "\n",
    "for threshold in tqdm(thresholds, desc=f\"Creating Cluster for threshold\"):\n",
    "    edge_list = create_edgelist(msd_small, threshold)\n",
    "    edge_lists[threshold] = edge_list\n",
    "    clusters[threshold] = find_components(edge_list)\n",
    "\n",
    "# Create Cluster Mapping\n",
    "id_2_cluster = {threshold: {} for threshold in clusters}\n",
    "for threshold in clusters:\n",
    "    id_2_cluster[threshold] = {\n",
    "        name: i + 1 for i, lst in enumerate(clusters[threshold]) for name in lst\n",
    "    }\n",
    "\n",
    "for threshold in id_2_cluster:\n",
    "    df[f\"cluster_{threshold}\"] = df.claim_minimal.map(id_2_cluster[threshold])\n",
    "    df[f\"cluster_{threshold}\"] = df[f\"cluster_{threshold}\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df with cluster columns\n",
    "df.to_csv(\"../Data/df_with_clusters_local_translated_with_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Fact-checks; Singleton Ratio over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_singleton_0.875\"] = df[\"cluster_0.875\"].map(lambda x: 1 if x == 0 else 0)\n",
    "df[\"datePublished\"] = pd.to_datetime(df.datePublished, errors=\"coerce\")\n",
    "\n",
    "fig, (ax2, ax1) = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "# Plot singleton ratio\n",
    "df[\n",
    "    (df[\"datePublished\"] > \"2019-01-01\") & (df[\"datePublished\"] < \"2024-12-01\")\n",
    "].set_index(\"datePublished\").resample(\"3M\")[\"is_singleton_0.875\"].mean().plot(\n",
    "    ax=ax1, marker=\"o\", color=\"blue\", alpha=1, linestyle=\"--\"\n",
    ")\n",
    "ax1.set_ylim(0.85, 0.96)\n",
    "ax1.set_ylabel(\"Singleton Ratio\")\n",
    "ax1.text(\n",
    "    -0.1,\n",
    "    1.05,\n",
    "    \"B\",\n",
    "    transform=ax1.transAxes,\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "# Plot the number of clusters\n",
    "df[\n",
    "    (df[\"datePublished\"] > \"2019-01-01\") & (df[\"datePublished\"] < \"2024-12-01\")\n",
    "].set_index(\"datePublished\").resample(\"3M\")[\"cluster_0.875\"].size().plot(\n",
    "    ax=ax2, color=\"red\", linestyle=\"--\", alpha=1, marker=\"o\"\n",
    ")\n",
    "ax2.set_ylabel(\"Number of Fact-checks\")\n",
    "ax2.text(\n",
    "    -0.1,\n",
    "    1.05,\n",
    "    \"A\",\n",
    "    transform=ax2.transAxes,\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "ax1.axvline(pd.to_datetime(\"2020-01-01\"), color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax1.text(\n",
    "    pd.to_datetime(\"2020-01-01\"),\n",
    "    0.94,\n",
    "    \"Jan. 2020\",\n",
    "    rotation=90,\n",
    "    verticalalignment=\"center\",\n",
    "    horizontalalignment=\"right\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax1.axvline(pd.to_datetime(\"2023-01-01\"), color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax1.text(\n",
    "    pd.to_datetime(\"2023-01-01\"),\n",
    "    0.94,\n",
    "    \"Jan. 2023\",\n",
    "    rotation=90,\n",
    "    verticalalignment=\"center\",\n",
    "    horizontalalignment=\"right\",\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "ax2.axvline(pd.to_datetime(\"2020-01-01\"), color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax2.text(\n",
    "    pd.to_datetime(\"2020-01-01\"),\n",
    "    2.5 * 10**4,\n",
    "    \"Jan. 2020\",\n",
    "    rotation=90,\n",
    "    verticalalignment=\"center\",\n",
    "    horizontalalignment=\"right\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax2.axvline(pd.to_datetime(\"2023-01-01\"), color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax2.text(\n",
    "    pd.to_datetime(\"2023-01-01\"),\n",
    "    2.5 * 10**4,\n",
    "    \"Jan. 2023\",\n",
    "    rotation=90,\n",
    "    verticalalignment=\"center\",\n",
    "    horizontalalignment=\"right\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax2.set_ylim(0, 3.1 * 10**4)\n",
    "ax1.set_xlabel(\"Date\")\n",
    "\n",
    "# set y-axis to percentage\n",
    "ax1.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "plt.savefig(\n",
    "    \"../Plots/Singleton_Ratio_and_Number_of_Fact-Checks_over_Time.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.savefig(\n",
    "    \"../Plots/Singleton_Ratio_and_Number_of_Fact-Checks_over_Time.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Nodes & Edges that are outside of observation window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Fact-Checks That Are between March 2020 and March 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    (df.datePublished < pd.to_datetime(\"2023-01-01\", utc=True))\n",
    "    & (df.datePublished > pd.to_datetime(\"2020-01-01\", utc=True))\n",
    "]\n",
    "cluster_cols = [col for col in df.columns if \"cluster_\" in col]\n",
    "df[cluster_cols] = df[cluster_cols].apply(\n",
    "    lambda col: col.replace(col.value_counts()[col.value_counts() == 1].index, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_claims = set(list(df.claim_minimal))\n",
    "\n",
    "for threshold in tqdm(edge_lists.keys()):\n",
    "    edge_lists[threshold] = [\n",
    "        (x[0], x[1])\n",
    "        for x in edge_lists[threshold]\n",
    "        if x[0] in present_claims and x[1] in present_claims\n",
    "    ]\n",
    "\n",
    "# Initialize dictionaries\n",
    "to_alter = {k: {} for k in thresholds}\n",
    "to_singleton = {k: [] for k in thresholds}\n",
    "\n",
    "# Check connectedness\n",
    "for threshold in thresholds:\n",
    "    cluster_dict_thresh = (\n",
    "        df.groupby(f\"cluster_{threshold}\")[\"claim_minimal\"].apply(list).to_dict()\n",
    "    )\n",
    "    max_cluster = max(cluster_dict_thresh.keys())\n",
    "\n",
    "    G = nx.Graph(edge_lists[threshold])\n",
    "\n",
    "    for cluster, nodes in cluster_dict_thresh.items():\n",
    "        if cluster == 0:\n",
    "            continue\n",
    "\n",
    "        # Create subgraph for this cluster\n",
    "        subG = G.subgraph(nodes)\n",
    "\n",
    "        # Find connected components in this subgraph\n",
    "        connected_components = list(nx.connected_components(subG))\n",
    "        if len(connected_components) == 0:\n",
    "            to_singleton[threshold].extend(nodes)\n",
    "        if len(connected_components) == 1:\n",
    "            continue\n",
    "        for component in connected_components:\n",
    "            if len(component) == 1:\n",
    "                to_singleton[threshold].extend(component)\n",
    "            else:\n",
    "                max_cluster += 1\n",
    "                to_alter[threshold][max_cluster] = list(component)\n",
    "\n",
    "# Update df\n",
    "for threshold in thresholds:\n",
    "    alter_map = {old: new for new, olds in to_alter[threshold].items() for old in olds}\n",
    "    singleton_set = set(to_singleton[threshold])\n",
    "\n",
    "    def update_cluster_id(row):\n",
    "        if row[\"claim_minimal\"] in singleton_set:\n",
    "            return 0\n",
    "        return alter_map.get(row[\"claim_minimal\"], row[f\"cluster_{threshold}\"])\n",
    "\n",
    "    df[f\"cluster_{threshold}\"] = df.apply(update_cluster_id, axis=1)\n",
    "\n",
    "present_claims = set(list(df.claim_minimal))\n",
    "for threshold in tqdm(edge_lists.keys()):\n",
    "    id_2_cluster = df.set_index(\"claim_minimal\")[f\"cluster_{threshold}\"].to_dict()\n",
    "    edge_lists[threshold] = [\n",
    "        (x[0], x[1])\n",
    "        for x in edge_lists[threshold]\n",
    "        if x[0] in present_claims and x[1] in present_claims\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary of ID -> Verdict for quick lookup\n",
    "verdicts = df.set_index(\"claim_minimal\").verdict.to_dict()\n",
    "# Get Probability distribution for verdict_values\n",
    "counts = Counter(verdicts.values())\n",
    "# Remove nan. We did not map all verdicts to a verdict value.\n",
    "counts.pop(np.nan)\n",
    "# Normalize to get probabilities\n",
    "verdict_probabilities = {k: v / sum(counts.values()) for k, v in counts.items()}\n",
    "# Map ID -> Language\n",
    "id_2_lang = df.set_index(\"claim_minimal\")[\"language\"].to_dict()\n",
    "\n",
    "\n",
    "def calc_measures(labels, just_two=False, random=False):\n",
    "    \"\"\"\n",
    "    Calculates the majority, entropy and gini for a list of labels.\n",
    "    We use this to calculate the measures for the clusters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list\n",
    "        List of labels.\n",
    "    just_two : bool, optional\n",
    "        If True, we map all labels to True or False. The default is False.\n",
    "    random : bool, optional\n",
    "        If True, we sample the labels from the distribution of verdicts. The default is False.\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        num_labels = len(labels)\n",
    "        labels = np.random.choice(\n",
    "            list(verdict_probabilities.keys()),\n",
    "            size=num_labels,\n",
    "            p=list(verdict_probabilities.values()),\n",
    "        )\n",
    "    else:\n",
    "        labels = [verdicts[x] for x in labels if x in verdicts and isinstance(x, str)]\n",
    "        labels = [x for x in labels if str(x) != \"nan\"]  # removing 'nan' values\n",
    "\n",
    "    if just_two:\n",
    "        labels = [\n",
    "            \"False\" if \"False\" in label else \"True\" for label in labels\n",
    "        ]  # mapping labels\n",
    "        assert len(set(labels)) <= 2, f\"Labels are not binary: {set(labels)}\"\n",
    "\n",
    "    if len(labels) == 0:\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    counter = Counter(labels)\n",
    "    total = len(labels)\n",
    "    majority = max(counter.values()) / total\n",
    "    label_counts = np.array(list(counter.values()))\n",
    "    probs = label_counts / total\n",
    "    ent = entropy(probs, base=2)\n",
    "    gini = 1 - sum((count / total) ** 2 for count in counter.values())\n",
    "    return majority, ent, gini, total\n",
    "\n",
    "\n",
    "def measure_cluster_consistency(cluster_dict, just_two=False, random=False):\n",
    "    \"\"\"\n",
    "    Wrap calc_measures to calculate the measures for a cluster_dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cluster_dict : dict\n",
    "        Dictionary mapping cluster_id -> list of ids.\n",
    "    just_two : bool, optional\n",
    "        If True, we map all labels to True or False. The default is False.\n",
    "    random : bool, optional\n",
    "        If True, we sample the labels from the distribution of verdicts. The default is False.\n",
    "    \"\"\"\n",
    "    majority_scores = []\n",
    "    entropy_scores = []\n",
    "    gini_scores = []\n",
    "    total_size = 0\n",
    "\n",
    "    for labels in cluster_dict:\n",
    "        majority, ent, gini, size = calc_measures(labels, just_two, random)\n",
    "        majority_scores.append(majority * size)\n",
    "        entropy_scores.append(ent * size)\n",
    "        gini_scores.append(gini * size)\n",
    "        total_size += size\n",
    "\n",
    "    average_majority = sum(majority_scores) / total_size\n",
    "    average_entropy = sum(entropy_scores) / total_size\n",
    "    average_gini = sum(gini_scores) / total_size\n",
    "\n",
    "    return average_majority, average_entropy, average_gini\n",
    "\n",
    "\n",
    "# Statistics that we want\n",
    "\n",
    "N_CLAIMS = df.shape[0]\n",
    "\n",
    "stats = {k: {} for k in thresholds}\n",
    "# Create Null Model =>  Randomly assign each claim to a language with associated probability\n",
    "lang_distr = pd.Series(id_2_lang.values()).value_counts(normalize=True).to_dict()\n",
    "for key in tqdm(stats.keys(), desc=\"Calculating Statistics\"):\n",
    "    num_per_cluster = [len(x) for x in clusters[key]]\n",
    "    langs = [list(map(id_2_lang.get, x)) for x in clusters[key] if len(x) > 1]\n",
    "    rand_langs = [\n",
    "        list(\n",
    "            np.random.choice(\n",
    "                list(lang_distr.keys()),\n",
    "                len(x),\n",
    "                p=list(lang_distr.values()),\n",
    "                replace=True,\n",
    "            )\n",
    "        )\n",
    "        for x in clusters[key]\n",
    "        if len(x) > 1\n",
    "    ]\n",
    "    # 1. Number of clusters\n",
    "    stats[key][\"num_cluster\"] = len(clusters[key])\n",
    "    # 2. Median Number of claims in each cluster (where the number of claims is > 1)\n",
    "    stats[key][\"median_num\"] = np.median([x for x in num_per_cluster if x > 1])\n",
    "    # 3. Mean Number of claims in each cluster (where the number of claims is > 1)\n",
    "    stats[key][\"mean_num\"] = np.mean([x for x in num_per_cluster if x > 1])\n",
    "    # 4. Number of claims in the largest cluster\n",
    "    stats[key][\"size_largest\"] = max(num_per_cluster)\n",
    "    # 5. % of Fact-checks that are singletons\n",
    "    N_NODES = len(set([item for sublist in clusters[key] for item in sublist]))\n",
    "    # Percentage Singletons\n",
    "    stats[key][\"perc_singletons\"] = (N_CLAIMS - N_NODES) / N_CLAIMS\n",
    "    # % of Clusters that are mono-lingual\n",
    "    stats[key][\"perc_mono\"] = sum([1 for x in langs if len(set(x)) == 1]) / len(langs)\n",
    "    stats[key][\"perc_mono_rand\"] = sum(\n",
    "        [1 for x in rand_langs if len(set(x)) == 1]\n",
    "    ) / len(rand_langs)\n",
    "    # % of Clusters that are two languages\n",
    "    stats[key][\"perc_two\"] = sum([1 for x in langs if len(set(x)) == 2]) / len(langs)\n",
    "    stats[key][\"perc_two_rand\"] = sum(\n",
    "        [1 for x in rand_langs if len(set(x)) == 2]\n",
    "    ) / len(rand_langs)\n",
    "\n",
    "    stats[key][\"perc_more\"] = sum([1 for x in langs if len(set(x)) >= 3]) / len(langs)\n",
    "    stats[key][\"perc_more_rand\"] = sum(\n",
    "        [1 for x in rand_langs if len(set(x)) >= 3]\n",
    "    ) / len(rand_langs)\n",
    "\n",
    "    stats_mode, stats_entropy, stats_gini = measure_cluster_consistency(clusters[key])\n",
    "    stats_mode_two, stats_entropy_two, stats_gini_two = measure_cluster_consistency(\n",
    "        clusters[key], just_two=True\n",
    "    )\n",
    "    stats_mode_rand, stats_entropy_rand, stats_gini_rand = measure_cluster_consistency(\n",
    "        clusters[key], random=True\n",
    "    )\n",
    "    stats_mode_two_rand, stats_entropy_two_rand, stats_gini_two_rand = (\n",
    "        measure_cluster_consistency(clusters[key], just_two=True, random=True)\n",
    "    )\n",
    "\n",
    "    stats[key][\"mode_rand\"] = stats_mode_rand\n",
    "    stats[key][\"entropy_rand\"] = stats_entropy_rand\n",
    "    stats[key][\"gini_rand\"] = stats_gini_rand\n",
    "\n",
    "    stats[key][\"just_two_mode_rand\"] = stats_mode_two_rand\n",
    "    stats[key][\"just_two_entropy_rand\"] = stats_entropy_two_rand\n",
    "    stats[key][\"just_two_gini_rand\"] = stats_gini_two_rand\n",
    "\n",
    "    stats[key][\"mode\"] = stats_mode\n",
    "    stats[key][\"entropy\"] = stats_entropy\n",
    "    stats[key][\"gini\"] = stats_gini\n",
    "\n",
    "    stats[key][\"just_two_mode\"] = stats_mode_two\n",
    "    stats[key][\"just_two_entropy\"] = stats_entropy_two\n",
    "    stats[key][\"just_two_gini\"] = stats_gini_two\n",
    "\n",
    "stats = pd.DataFrame(stats).T.reset_index().rename(columns={\"index\": \"threshold\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Consistency Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = stats.set_index(\"threshold\")[\"mode\"].round(4) * 100\n",
    "two_mode = stats.set_index(\"threshold\")[\"just_two_mode\"].round(4) * 100\n",
    "mode_rand = stats.set_index(\"threshold\")[\"mode_rand\"].round(4) * 100\n",
    "two_mode_rand = stats.set_index(\"threshold\")[\"just_two_mode_rand\"].round(4) * 100\n",
    "\n",
    "\n",
    "def print_latex_table():\n",
    "    # Convert all numerical values to strings with proper formatting\n",
    "    str_thresholds = list(map(str, thresholds))\n",
    "    str_two_mode = list(map(lambda x: f\"{x:.2f}\", two_mode))\n",
    "    str_two_mode_rand = list(map(lambda x: f\"{x:.2f}\", two_mode_rand))\n",
    "    str_four_mode = list(map(lambda x: f\"{x:.2f}\", mode))\n",
    "    str_four_mode_rand = list(map(lambda x: f\"{x:.2f}\", mode_rand))\n",
    "\n",
    "    # Start of the table\n",
    "    print(\"\\\\begin{table}[H]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\begin{tabular}{l\" + \"c\" * len(thresholds) + \"}\")\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "    # Header\n",
    "    header_row = \" & \".join([\"\"] + str_thresholds) + \" \\\\\\\\ \\\\hline\"\n",
    "    print(header_row)\n",
    "\n",
    "    # Two Mode Rows\n",
    "    two_mode_row = \"Two Mode & \" + \" & \".join(str_two_mode) + \" \\\\\\\\\"\n",
    "    two_mode_rand_row = (\n",
    "        \"\\\\small{(Two Mode Rand)} & \"\n",
    "        + \" & \".join(f\"\\\\small{{({x})}}\" for x in str_two_mode_rand)\n",
    "        + \" \\\\\\\\ \\\\hline\"\n",
    "    )\n",
    "    print(two_mode_row)\n",
    "    print(two_mode_rand_row)\n",
    "\n",
    "    # Four Mode Rows\n",
    "    four_mode_row = \"Four Mode & \" + \" & \".join(str_four_mode) + \" \\\\\\\\\"\n",
    "    four_mode_rand_row = (\n",
    "        \"\\\\small{(Four Mode Rand)} & \"\n",
    "        + \" & \".join(f\"\\\\small{{({x})}}\" for x in str_four_mode_rand)\n",
    "        + \" \\\\\\\\ \\\\hline\"\n",
    "    )\n",
    "    print(four_mode_row)\n",
    "    print(four_mode_rand_row)\n",
    "\n",
    "    # End of the table\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\caption{Your Table Title}\")\n",
    "    print(\"\\\\label{tab:your_label}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "\n",
    "\n",
    "# Call the function to print the LaTeX code\n",
    "print_latex_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage of clusters by Number of languages across thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Both line and scatter plot\n",
    "plt.plot(stats[\"threshold\"], stats[\"perc_mono\"], label=\"Mono-lingual\")\n",
    "plt.plot(stats[\"threshold\"], stats[\"perc_two\"], label=\"Two-lingual\")\n",
    "# plt.plot(stats[\"threshold\"], stats[\"perc_three\"], label=\"Three-lingual\")\n",
    "plt.plot(stats[\"threshold\"], stats[\"perc_more\"], label=\"More than three-lingual\")\n",
    "# Scatter plot\n",
    "plt.scatter(stats[\"threshold\"], stats[\"perc_mono\"], label=None)\n",
    "plt.scatter(stats[\"threshold\"], stats[\"perc_two\"], label=None)\n",
    "# plt.scatter(stats[\"threshold\"], stats[\"perc_three\"], label=None)\n",
    "plt.scatter(stats[\"threshold\"], stats[\"perc_more\"], label=None)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "# Foramt y-axis to Percentage\n",
    "# Set y-axis log2\n",
    "plt.yscale(\"log\", base=2)\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "# Set title\n",
    "plt.title(\n",
    "    \"Percentage of clusters by number of languages very stable across Threshholds\"\n",
    ")\n",
    "plt.xticks(np.arange(0.75, 0.975, 0.025))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.savefig(\"../Plots/perc_clusters_by_lang.png\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actual vs Random Percentage of Clusters by Number of Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "# Add 45 degree line\n",
    "plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Shade area below 45 degree line with vertical stripes\n",
    "plt.fill_between(\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 0],\n",
    "    hatch=\"/\",\n",
    "    facecolor=\"lightblue\",\n",
    "    edgecolor=\"blue\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Shade area above 45 degree line with horizontal stripes\n",
    "plt.fill_between(\n",
    "    [0, 1], [0, 1], [1, 1], hatch=\"-\", facecolor=\"lightpink\", edgecolor=\"red\", alpha=0.2\n",
    ")\n",
    "\n",
    "# Draw arrows between points\n",
    "for i in range(stats.shape[0] - 2):\n",
    "    for var in [\"mono\", \"two\", \"more\"]:\n",
    "        plt.arrow(\n",
    "            stats[\"perc_\" + var + \"_rand\"][i],\n",
    "            stats[\"perc_\" + var][i],\n",
    "            stats[\"perc_\" + var + \"_rand\"][i + 1] - stats[\"perc_\" + var + \"_rand\"][i],\n",
    "            stats[\"perc_\" + var][i + 1] - stats[\"perc_\" + var][i],\n",
    "            head_width=0.01,\n",
    "            head_length=0.02,\n",
    "            fc=\"k\",\n",
    "            ec=\"k\",\n",
    "            length_includes_head=True,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "# Set marker styles and colors\n",
    "mono_marker = \"D\"\n",
    "two_marker = \"o\"\n",
    "more_marker = \"^\"\n",
    "\n",
    "mono_color = \"blue\"\n",
    "two_color = \"red\"\n",
    "more_color = \"lightgreen\"\n",
    "\n",
    "plt.scatter(\n",
    "    stats[\"perc_mono_rand\"].iloc[:-1],\n",
    "    stats[\"perc_mono\"].iloc[:-1],\n",
    "    label=\"Mono-lingual\",\n",
    "    marker=mono_marker,\n",
    "    color=mono_color,\n",
    "    edgecolors=\"black\",\n",
    ")\n",
    "plt.scatter(\n",
    "    stats[\"perc_two_rand\"].iloc[:-1],\n",
    "    stats[\"perc_two\"].iloc[:-1],\n",
    "    label=\"Two Languages\",\n",
    "    marker=two_marker,\n",
    "    color=two_color,\n",
    "    edgecolors=\"black\",\n",
    ")\n",
    "plt.scatter(\n",
    "    stats[\"perc_more_rand\"].iloc[:-1],\n",
    "    stats[\"perc_more\"].iloc[:-1],\n",
    "    label=\"More than Three Languages\",\n",
    "    marker=more_marker,\n",
    "    color=more_color,\n",
    "    edgecolors=\"black\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Expected\", fontsize=14)\n",
    "plt.ylabel(\"Observed\", fontsize=14)\n",
    "\n",
    "# Format axes to percentage\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper right\", borderaxespad=0)\n",
    "plt.xlim(0, 0.8)\n",
    "plt.ylim(0, 0.8)\n",
    "\n",
    "# Add x-axis tick marks at regular intervals\n",
    "plt.xticks(np.arange(0, 0.9, 0.1), fontsize=12)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.grid(False)\n",
    "plt.savefig(\"../Plots/actual_vs_random_accessible.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"../Plots/actual_vs_random_accessible.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold vs number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold vs largest & perc_singletons on the same plot with two y-axis\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(stats.threshold, stats.size_largest)\n",
    "\n",
    "plt.xlabel(\"Threshold\")\n",
    "# Set ylabel 1 to blue\n",
    "ax1.set_ylabel(\"Size of the Largest Cluster\", color=\"blue\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2 = ax1.twinx()\n",
    "plt.plot(stats.threshold, stats.perc_singletons, color=\"red\")\n",
    "\n",
    "# Set ylabel 2 to red\n",
    "ax2.set_ylabel(\"Percentage of Singletons\", color=\"red\")\n",
    "# Percentage Formatter ax2\n",
    "ax2.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "# Remove grid\n",
    "ax1.grid(False)\n",
    "ax2.grid(False)\n",
    "# Set title\n",
    "plt.title(\"Threshold vs Number of Clusters & Percentage of Singletons\")\n",
    "# save plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Inter and Intra Cluster Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path, ids_order=ORIGINAL_IDS):\n",
    "    \"\"\"Load embeddings from a numpy file.\"\"\"\n",
    "    try:\n",
    "        embeddings_dict = np.load(file_path, allow_pickle=True).item()\n",
    "        embeddings_ordered = {\n",
    "            id: embeddings_dict[id] for id in ids_order if id in embeddings_dict\n",
    "        }\n",
    "        return embeddings_ordered\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load embeddings from {file_path} with error {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_all_embeddings(folder_path):\n",
    "    \"\"\"Load all embeddings from a directory.\"\"\"\n",
    "    embeddings_dict = {}\n",
    "    # Get the list of filenames and sort them by the number included in the filename\n",
    "    filenames = sorted(\n",
    "        [x for x in os.listdir(folder_path) if not x.startswith(\".\")],\n",
    "        key=lambda x: int(re.search(r\"\\d+\", x).group()),\n",
    "    )\n",
    "\n",
    "    for file_name in filenames:\n",
    "        if file_name.endswith(\".npy\"):\n",
    "            embeddings_dict.update(\n",
    "                load_embeddings(os.path.join(folder_path, file_name))\n",
    "            )\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "# Load all Embeddings\n",
    "embeddings = load_all_embeddings(\"../Data/embeddings/\")\n",
    "# Check that all embeddings were loaded\n",
    "variance_calc = deepcopy(clusters)\n",
    "\n",
    "\n",
    "# Calculate the variance of a cluster\n",
    "def intra_cluster_variance(cluster):\n",
    "    \"\"\"\n",
    "    Calculate the variance of a cluster\n",
    "    \"\"\"\n",
    "    # Get the embeddings of the claims in the cluster\n",
    "    cluster_embeddings = [embeddings[claim] for claim in cluster]\n",
    "    # Calculate the variance of the embeddings\n",
    "    variance = np.var(cluster_embeddings, axis=0)\n",
    "    # Return the sum of the variance\n",
    "    return np.sum(variance)\n",
    "\n",
    "\n",
    "# Filter each key in variance_calc to only include clusters with at least 2 claims\n",
    "for key in variance_calc:\n",
    "    variance_calc[key] = [x for x in variance_calc[key] if len(x) > 1]\n",
    "# Turn each key into dict with ID = threshold_index\n",
    "for key in variance_calc:\n",
    "    variance_calc[key] = {f\"{key}_{i}\": x for i, x in enumerate(variance_calc[key])}\n",
    "variance_stats = {}\n",
    "\n",
    "for threshold in tqdm(thresholds, desc=\"Calculating Variance\"):\n",
    "    variance_stats[threshold] = {}\n",
    "    variance_stats[threshold][\"intra_variances\"], variance_stats[threshold][\"sizes\"] = (\n",
    "        {},\n",
    "        {},\n",
    "    )\n",
    "    variance_stats[threshold][\"intra_variances\"] = {\n",
    "        k: intra_cluster_variance(v) for k, v in variance_calc[threshold].items()\n",
    "    }\n",
    "    variance_stats[threshold][\"sizes\"] = {\n",
    "        k: len(v) for k, v in variance_calc[threshold].items()\n",
    "    }\n",
    "\n",
    "\n",
    "def centroid(cluster):\n",
    "    \"\"\"\n",
    "    Calculate the centroid of a cluster\n",
    "    \"\"\"\n",
    "    # Get the embeddings of the claims in the cluster\n",
    "    cluster_embeddings = [embeddings[claim] for claim in cluster]\n",
    "    # Calculate the centroid of the embeddings\n",
    "    centroid = np.mean(cluster_embeddings, axis=0)\n",
    "    # Return the centroid\n",
    "    return centroid\n",
    "\n",
    "\n",
    "def sample_combinations(cluster_tuples, n=10000):\n",
    "    \"\"\"\n",
    "    Create a list of all combinations of clusters.\n",
    "    If the number of combinations is too large, we sample n combinations.\n",
    "    \"\"\"\n",
    "    # n! / (2! * (n - 2)!) = 10000\n",
    "    # n**2 - n - 20.000 = 0 => ~142\n",
    "    if len(cluster_tuples) <= 142:\n",
    "        combs = list(combinations(cluster_tuples.items(), 2))\n",
    "        # Filter combs to only include tuples where i != j\n",
    "        combs = [x for x in combs if x[0][0] != x[1][0]]\n",
    "    else:\n",
    "        combs = []\n",
    "        keys = list(cluster_tuples.keys())\n",
    "        for i in range(n):\n",
    "            indeces = random.sample(keys, 2)\n",
    "            combs.append((cluster_tuples[indeces[0]], cluster_tuples[indeces[1]]))\n",
    "        combs = [x for x in combs if x[0][0] != x[1][0]]\n",
    "    return combs\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "# Create a dict with the centroids of each cluster\n",
    "centroids = {}\n",
    "for threshold in tqdm(thresholds, desc=\"Creating centroids\"):\n",
    "    centroids[threshold] = {}\n",
    "    for key in variance_calc[threshold]:\n",
    "        # Remove clusters with only 1 claim\n",
    "        if len(variance_calc[threshold][key]) > 1:\n",
    "            centroids[threshold][key] = centroid(variance_calc[threshold][key])\n",
    "\n",
    "# Create a dict with all centroid tuples where i != j\n",
    "centroid_tuples = {}\n",
    "for threshold in tqdm(thresholds, desc=\"Creating centroid tuples\"):\n",
    "    centroid_tuples[threshold] = {}\n",
    "    centroid_tuples[threshold] = sample_combinations(centroids[threshold], n=1_000)\n",
    "\n",
    "# Calculate the distance between each centroid (cosine similarity)\n",
    "centroid_distances = {}\n",
    "for threshold in tqdm(thresholds, desc=\"Calculating centroid distances\"):\n",
    "    centroid_distances[threshold] = {}\n",
    "    # Use cosine distance\n",
    "    centroid_distances[threshold] = [\n",
    "        cosine_similarity(np.array(x[0]).reshape(1, -1), np.array(x[1]).reshape(1, -1))[\n",
    "            0\n",
    "        ][0]\n",
    "        for x in centroid_tuples[threshold]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Intra Cluster Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate avg_intra_variances for each threshold\n",
    "avg_intra_variances = [\n",
    "    np.mean(list(variance_stats[threshold][\"intra_variances\"].values()))\n",
    "    for threshold in thresholds\n",
    "]\n",
    "# Calculate avg_inter_variances for each threshold\n",
    "avg_inter_variances = [\n",
    "    np.mean(np.abs(centroid_distances[threshold])) for threshold in thresholds\n",
    "]\n",
    "\n",
    "# Find the threshold with minimum avg_intra_variance and maximum avg_inter_variance\n",
    "min_avg_intra_variance = min(avg_intra_variances)\n",
    "min_threshold_intra = thresholds[avg_intra_variances.index(min_avg_intra_variance)]\n",
    "\n",
    "max_avg_inter_variance = max(avg_inter_variances)\n",
    "max_threshold_inter = thresholds[avg_inter_variances.index(max_avg_inter_variance)]\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot for Average Intra Cluster Variance\n",
    "for threshold, avg_variance in zip(thresholds, avg_intra_variances):\n",
    "    color = \"red\" if threshold == min_threshold_intra else \"grey\"\n",
    "    ax[0].bar(\n",
    "        x=threshold, height=avg_variance, width=0.02, color=color, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"Average Intra Cluster Variance\", fontsize=14)\n",
    "\n",
    "# Plot for Average Inter Cluster Distance\n",
    "for threshold, avg_distance in zip(thresholds, avg_inter_variances):\n",
    "    color = \"red\" if threshold == max_threshold_inter else \"grey\"\n",
    "    ax[1].bar(\n",
    "        x=threshold, height=avg_distance, width=0.02, color=color, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "ax[1].set_xlabel(\"Thresholds\")\n",
    "ax[1].set_ylabel(\"Average Inter Cluster Distance\", fontsize=14)\n",
    "\n",
    "# Add Labels (A / B) - bold\n",
    "ax[0].text(\n",
    "    -0.075,\n",
    "    1.07,\n",
    "    \"A\",\n",
    "    transform=ax[0].transAxes,\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "ax[1].text(\n",
    "    -0.075,\n",
    "    1.07,\n",
    "    \"B\",\n",
    "    transform=ax[1].transAxes,\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "# set x-ticks\n",
    "ax[0].set_xticks(thresholds)\n",
    "ax[1].set_xticks(thresholds)\n",
    "\n",
    "plt.savefig(\"../Plots/avg_intra_inter_variance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"../Plots/avg_intra_inter_variance.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries\n",
    "to_alter = {k: {} for k in thresholds}\n",
    "to_singleton = {k: [] for k in thresholds}\n",
    "\n",
    "# Check connectedness\n",
    "for threshold in thresholds:\n",
    "    cluster_dict_thresh = (\n",
    "        df.groupby(f\"cluster_{threshold}\")[\"claim_minimal\"].apply(list).to_dict()\n",
    "    )\n",
    "    max_cluster = max(cluster_dict_thresh.keys())\n",
    "\n",
    "    G = nx.Graph(edge_lists[threshold])\n",
    "\n",
    "    for cluster, nodes in cluster_dict_thresh.items():\n",
    "        if cluster == 0:\n",
    "            continue\n",
    "\n",
    "        # Create subgraph for this cluster\n",
    "        subG = G.subgraph(nodes)\n",
    "\n",
    "        # Find connected components in this subgraph\n",
    "        connected_components = list(nx.connected_components(subG))\n",
    "        if len(connected_components) == 0:\n",
    "            to_singleton[threshold].extend(nodes)\n",
    "        if len(connected_components) == 1:\n",
    "            continue\n",
    "        for component in connected_components:\n",
    "            if len(component) == 1:\n",
    "                to_singleton[threshold].extend(component)\n",
    "            else:\n",
    "                max_cluster += 1\n",
    "                to_alter[threshold][max_cluster] = list(component)\n",
    "\n",
    "# Update df\n",
    "for threshold in thresholds:\n",
    "    alter_map = {old: new for new, olds in to_alter[threshold].items() for old in olds}\n",
    "    singleton_set = set(to_singleton[threshold])\n",
    "\n",
    "    def update_cluster_id(row):\n",
    "        if row[\"claim_minimal\"] in singleton_set:\n",
    "            return 0\n",
    "        return alter_map.get(row[\"claim_minimal\"], row[f\"cluster_{threshold}\"])\n",
    "\n",
    "    df[f\"cluster_{threshold}\"] = df.apply(update_cluster_id, axis=1)\n",
    "\n",
    "present_claims = set(list(df.claim_minimal))\n",
    "for threshold in tqdm(edge_lists.keys()):\n",
    "    id_2_cluster = df.set_index(\"claim_minimal\")[f\"cluster_{threshold}\"].to_dict()\n",
    "    edge_lists[threshold] = [\n",
    "        (x[0], x[1])\n",
    "        for x in edge_lists[threshold]\n",
    "        if x[0] in present_claims and x[1] in present_claims\n",
    "    ]  # and id_2_cluster[x[0]] != 0 and id_2_cluster[x[1]] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "all_diffs = {}\n",
    "present_claims = set(list(df.claim_minimal))\n",
    "\n",
    "# Adjusted list of thresholds for the plot\n",
    "adjusted_thresholds = [0.8, 0.825, 0.85, 0.875, 0.9]\n",
    "\n",
    "for threshold in tqdm(\n",
    "    [k for k in edge_lists.keys() if k in adjusted_thresholds],\n",
    "    desc=f\"Calculating time differences\",\n",
    "):\n",
    "    ana_cl = edge_lists[threshold]\n",
    "    ana_cl_dates = []\n",
    "\n",
    "    # Remove edges that contain nodes outside of the observation period\n",
    "    ana_cl = [x for x in ana_cl if x[0] in present_claims and x[1] in present_claims]\n",
    "\n",
    "    for edges in tqdm(ana_cl, leave=False):\n",
    "        ana_cl_dates.append([dates[x] for x in edges])\n",
    "\n",
    "    diffs = [abs(x[0] - x[1]).days for x in ana_cl_dates]\n",
    "    all_diffs[threshold] = diffs\n",
    "\n",
    "    perc = pd.Series([np.mean(pd.Series(diffs) <= x) for x in range(1, 30)])\n",
    "    plt.plot(perc, label=f\"{threshold}\")\n",
    "\n",
    "    # Make the label at the end of each line much bigger and include \"Threshold: \"\n",
    "    if threshold == 0.9:\n",
    "        plt.text(\n",
    "            len(perc) - 3,\n",
    "            perc.iloc[-1],\n",
    "            f\"Threshold\\n{threshold}\",\n",
    "            color=plt.gca().lines[-1].get_color(),\n",
    "            fontsize=12,\n",
    "            verticalalignment=\"center\",\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "        )\n",
    "    elif threshold == 0.8:\n",
    "        plt.text(\n",
    "            len(perc) - 3,\n",
    "            perc.iloc[-1] - 0.02,\n",
    "            f\"Threshold\\n{threshold}\",\n",
    "            color=plt.gca().lines[-1].get_color(),\n",
    "            fontsize=12,\n",
    "            verticalalignment=\"center\",\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "        )\n",
    "    else:\n",
    "        plt.text(\n",
    "            len(perc) - 3,\n",
    "            perc.iloc[-1] - 0.03,\n",
    "            f\"Threshold\\n{threshold}\",\n",
    "            color=plt.gca().lines[-1].get_color(),\n",
    "            fontsize=12,\n",
    "            verticalalignment=\"center\",\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "        )\n",
    "\n",
    "    for x in [7, 21]:\n",
    "        plt.annotate(\n",
    "            f\"{perc[x]*100:.2f}%\",\n",
    "            (x, perc[x]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 10),\n",
    "            ha=\"center\",\n",
    "            color=plt.gca().lines[-1].get_color(),\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "        )\n",
    "\n",
    "# Vertical lines and texts\n",
    "for x in [7, 14, 21, 28]:\n",
    "    plt.axvline(x=x, color=\"grey\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.text(\n",
    "        x,\n",
    "        1.01,\n",
    "        f\"{x} Days\",\n",
    "        transform=plt.gca().get_xaxis_transform(),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=8,\n",
    "        color=\"grey\",\n",
    "    )\n",
    "\n",
    "# Modify y-axis to show percentage\n",
    "plt.gca().set_yticklabels([\"{:.0f}%\".format(x * 100) for x in plt.gca().get_yticks()])\n",
    "\n",
    "# Improve x-axis\n",
    "plt.xticks(range(0, 30, 1))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Days\", fontsize=12)\n",
    "plt.ylabel(\"Percentage of Edges with Time Difference â‰¤ x\", fontsize=12)\n",
    "plt.savefig(\"../Plots/percentage_edges_time_diff.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"../Plots/percentage_edges_time_diff.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_time = {}\n",
    "\n",
    "for threshhold in [0.75, 0.775, 0.8, 0.825, 0.85, 0.875, 0.9, 0.925, 0.95]:\n",
    "    # Get the number of claims in each cluster\n",
    "    clusters = df[f\"cluster_{threshhold}\"].value_counts()\n",
    "    # We remove clusters with only 1 or 2 claims. 2 is trivial, as similarity is necessarily equal to the threshold.\n",
    "    clusters_to_incl = clusters[clusters > 2].index.to_list()\n",
    "    # Filter df to only include claims in clusters with more than 2 claims\n",
    "    small = df[\n",
    "        (df[f\"cluster_{threshhold}\"].isin(clusters_to_incl))\n",
    "        & (df[f\"cluster_{threshhold}\"] != 0)\n",
    "    ]\n",
    "    # Get the dates of each claim\n",
    "    dates_cluster = df.set_index(\"claim_minimal\")[\"datePublished\"].to_dict()\n",
    "\n",
    "    all_simis = []\n",
    "    for cluster in tqdm(\n",
    "        small[f\"cluster_{threshhold}\"].unique(),\n",
    "        desc=f\"Calculating similarities for threshold {threshhold}\",\n",
    "    ):\n",
    "        if cluster == 0:\n",
    "            # Skip singletons\n",
    "            continue\n",
    "\n",
    "        # Get all claims in the cluster\n",
    "        claims_in_cluster = small.loc[\n",
    "            small[f\"cluster_{threshhold}\"] == cluster, \"claim_minimal\"\n",
    "        ]\n",
    "\n",
    "        if claims_in_cluster.shape[0] > 1000:  # Check that cluster is not too large\n",
    "            claims_in_cluster = claims_in_cluster.sample(1000).to_list()\n",
    "\n",
    "        # Load embeddings for claims in cluster\n",
    "        embeddings_cluster = [embeddings[x] for x in claims_in_cluster]\n",
    "        # Create a dict mapping index to claim (?)\n",
    "        index_to_claim = (\n",
    "            small.loc[small.claim_minimal.isin(claims_in_cluster), \"claim_minimal\"]\n",
    "            .reset_index(drop=True)\n",
    "            .to_dict()\n",
    "        )\n",
    "        # Calculate cosine_similarity\n",
    "        cosine_similarity_df = cosine_similarity(embeddings_cluster)\n",
    "        # Flatten and join with dates\n",
    "        cosine_similarity_df = pd.DataFrame(cosine_similarity_df).stack().reset_index()\n",
    "        cosine_similarity_df.columns = [\"claim_1\", \"claim_2\", \"cosine_similarity\"]\n",
    "        cosine_similarity_df[\"claim_1\"] = cosine_similarity_df[\"claim_1\"].map(\n",
    "            index_to_claim\n",
    "        )\n",
    "        cosine_similarity_df[\"claim_2\"] = cosine_similarity_df[\"claim_2\"].map(\n",
    "            index_to_claim\n",
    "        )\n",
    "        cosine_similarity_df[\"date_1\"] = cosine_similarity_df[\"claim_1\"].apply(\n",
    "            lambda x: dates_cluster[x]\n",
    "        )\n",
    "        cosine_similarity_df[\"date_2\"] = cosine_similarity_df[\"claim_2\"].apply(\n",
    "            lambda x: dates_cluster[x]\n",
    "        )\n",
    "        # Remove all directly connected claims. Any claim above the threshold is directly connected to itself.\n",
    "        all_simis.append(\n",
    "            cosine_similarity_df[cosine_similarity_df[\"cosine_similarity\"] < threshhold]\n",
    "        )\n",
    "    # Concatenate all dataframes\n",
    "    time_df = pd.concat(all_simis)\n",
    "    # Remove all claims where date_1 > date_2 (?)\n",
    "    time_df = time_df[time_df[\"date_2\"] > time_df[\"date_1\"]]\n",
    "    # Calculate the time difference\n",
    "    time_df[\"time_difference\"] = (time_df[\"date_2\"] - time_df[\"date_1\"]).dt.days\n",
    "    clustered_time[threshhold] = time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for thresh in clustered_time:\n",
    "    pl_df = clustered_time[thresh]\n",
    "    pl_df[\"day\"] = pl_df[\"time_difference\"] // 1\n",
    "    # Only calculate for the first 12 months\n",
    "    pl_df = pl_df[pl_df[\"day\"] < 365]\n",
    "    # At least 30 datapoints per month\n",
    "    pl_df = pl_df.groupby(\"day\").filter(lambda x: len(x) > 10)\n",
    "    # Calculate mean and variance\n",
    "    pl_df_mean = pl_df.groupby(\"day\")[\"cosine_similarity\"].mean().reset_index()\n",
    "    pl_df_var = pl_df.groupby(\"day\")[\"cosine_similarity\"].var().reset_index()\n",
    "\n",
    "    if thresh > 0.875:\n",
    "        print(f\"Skipping {thresh}\")\n",
    "        continue\n",
    "\n",
    "    pl_df_mean = pl_df_mean.dropna()\n",
    "    pl_df_var = pl_df_var.dropna()\n",
    "    pl_df_mean[\"day\"] = pl_df_mean[\"day\"].astype(float)\n",
    "    pl_df_var[\"day\"] = pl_df_var[\"day\"].astype(float)\n",
    "    pl_df_mean[\"cosine_similarity\"] = pl_df_mean[\"cosine_similarity\"].astype(float)\n",
    "    pl_df_var[\"cosine_similarity\"] = pl_df_var[\"cosine_similarity\"].astype(float)\n",
    "\n",
    "    # Plot the mean line\n",
    "    plt.plot(pl_df_mean[\"day\"], pl_df_mean[\"cosine_similarity\"], label=thresh)\n",
    "\n",
    "    # Calculate bounds for fill_between\n",
    "    lower = np.array(pl_df_mean[\"cosine_similarity\"] - pl_df_var[\"cosine_similarity\"])\n",
    "    upper = np.array(pl_df_mean[\"cosine_similarity\"] + pl_df_var[\"cosine_similarity\"])\n",
    "    days = np.array(pl_df_mean[\"day\"])\n",
    "\n",
    "    plt.fill_between(days, lower, upper, alpha=0.2)\n",
    "\n",
    "    # Put label at the end of the line\n",
    "    last_day = days[-1]\n",
    "    last_cosine_similarity = pl_df_mean[\"cosine_similarity\"].iloc[-1]\n",
    "    if thresh != 0.75:\n",
    "        plt.text(\n",
    "            last_day,\n",
    "            last_cosine_similarity,\n",
    "            f\" {thresh}\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=14,\n",
    "            color=plt.gca().lines[-1].get_color(),\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "        )\n",
    "    else:\n",
    "        plt.text(\n",
    "            last_day,\n",
    "            last_cosine_similarity - 0.02,\n",
    "            f\"{thresh}\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=14,\n",
    "            color=plt.gca().lines[-1].get_color(),\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "        )\n",
    "\n",
    "plt.xlim(0, 365)\n",
    "# plt.legend(loc=\"upper right\", title=\"Threshold\")\n",
    "plt.xlabel(\"Days\", fontsize=14)\n",
    "plt.ylabel(\"Average similarity\", fontsize=14)\n",
    "plt.savefig(\n",
    "    \"../Plots/avg_cosine_similarity_time_diff.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.savefig(\"../Plots/avg_cosine_similarity_time_diff.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot for the threshold 0.875\n",
    "plt_df_mean = (\n",
    "    clustered_time[0.875]\n",
    "    .groupby(\"time_difference\")[\"cosine_similarity\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "plt_df_var = (\n",
    "    clustered_time[0.875]\n",
    "    .groupby(\"time_difference\")[\"cosine_similarity\"]\n",
    "    .var()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ax1.plot(\n",
    "    plt_df_mean[\"time_difference\"], plt_df_mean[\"cosine_similarity\"], label=\"0.875\"\n",
    ")\n",
    "\n",
    "# Calculations for the fill_between\n",
    "plt_df_mean = plt_df_mean.dropna()\n",
    "plt_df_var = plt_df_var.dropna()\n",
    "\n",
    "plt_df_mean[\"time_difference\"] = plt_df_mean[\"time_difference\"].astype(float)\n",
    "plt_df_var[\"time_difference\"] = plt_df_var[\"time_difference\"].astype(float)\n",
    "\n",
    "plt_df_mean[\"cosine_similarity\"] = plt_df_mean[\"cosine_similarity\"].astype(float)\n",
    "plt_df_var[\"cosine_similarity\"] = plt_df_var[\"cosine_similarity\"].astype(float)\n",
    "\n",
    "upper = np.array(plt_df_mean[\"cosine_similarity\"] + plt_df_var[\"cosine_similarity\"])\n",
    "lower = np.array(plt_df_mean[\"cosine_similarity\"] - plt_df_var[\"cosine_similarity\"])\n",
    "days = np.array(plt_df_mean[\"time_difference\"])\n",
    "\n",
    "ax1.fill_between(days, lower, upper, alpha=0.2)\n",
    "ax1.set_xlim(0, 30)\n",
    "ax1.set_ylim(0.73, 0.82)\n",
    "y_change = {0.75: -0.04, 0.775: 0, 0.8: 0.01, 0.825: 0, 0.85: 0.03}\n",
    "\n",
    "# Other thresholds\n",
    "for threshold in clustered_time.keys():\n",
    "    if threshold >= 0.875:\n",
    "        continue\n",
    "\n",
    "    plt_df_mean = (\n",
    "        clustered_time[threshold]\n",
    "        .groupby(\"time_difference\")[\"cosine_similarity\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    plt_df_var = (\n",
    "        clustered_time[threshold]\n",
    "        .groupby(\"time_difference\")[\"cosine_similarity\"]\n",
    "        .var()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    plt_df_mean = plt_df_mean.dropna()\n",
    "    plt_df_var = plt_df_var.dropna()\n",
    "\n",
    "    plt_df_mean[\"time_difference\"] = plt_df_mean[\"time_difference\"].astype(float)\n",
    "    plt_df_var[\"time_difference\"] = plt_df_var[\"time_difference\"].astype(float)\n",
    "\n",
    "    plt_df_mean[\"cosine_similarity\"] = plt_df_mean[\"cosine_similarity\"].astype(float)\n",
    "    plt_df_var[\"cosine_similarity\"] = plt_df_var[\"cosine_similarity\"].astype(float)\n",
    "\n",
    "    ax2.plot(\n",
    "        plt_df_mean[\"time_difference\"],\n",
    "        plt_df_mean[\"cosine_similarity\"],\n",
    "        label=threshold,\n",
    "    )\n",
    "\n",
    "    upper = np.array(plt_df_mean[\"cosine_similarity\"] + plt_df_var[\"cosine_similarity\"])\n",
    "    lower = np.array(plt_df_mean[\"cosine_similarity\"] - plt_df_var[\"cosine_similarity\"])\n",
    "    days = np.array(plt_df_mean[\"time_difference\"])\n",
    "    plt_df_mean[\"cosine_similarity\"] = np.array(plt_df_mean[\"cosine_similarity\"])\n",
    "    ax2.fill_between(days, lower, upper, alpha=0.2)\n",
    "    last_day = 30\n",
    "    last_cosine_similarity = plt_df_mean[\"cosine_similarity\"][30]\n",
    "\n",
    "    ax2.text(\n",
    "        last_day,\n",
    "        last_cosine_similarity + y_change[threshold],\n",
    "        f\" {threshold}\",\n",
    "        verticalalignment=\"center\",\n",
    "        fontsize=14,\n",
    "        color=plt.gca().lines[-1].get_color(),\n",
    "        bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "    )\n",
    "\n",
    "\n",
    "ax2.set_xlim(0, 30)\n",
    "ax2.set_ylim(0.35, 0.75)\n",
    "# ax2.legend(title='Threshold', loc='upper right')\n",
    "ax1.set_xlabel(\"Timedifference in Days\", fontsize=14)\n",
    "ax2.set_xlabel(\"Timedifference in Days\", fontsize=14)\n",
    "ax1.set_ylabel(\"Cosine Similarity (Thresh. 0.875)\", fontsize=14)\n",
    "ax2.set_ylabel(\"Cosine Similarity (Other Thresh.)\", fontsize=14)\n",
    "plt.tight_layout(\n",
    "    rect=[0, 0.03, 1, 0.95]\n",
    ")  # Adjust the layout to make room for the suptitle\n",
    "\n",
    "# Add Labels to the Panels (A / B) - bold\n",
    "ax1.text(\n",
    "    -0.075,\n",
    "    1.07,\n",
    "    \"A\",\n",
    "    transform=ax1.transAxes,\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "ax2.text(\n",
    "    -0.075,\n",
    "    1.07,\n",
    "    \"B\",\n",
    "    transform=ax2.transAxes,\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "# Full Grid\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"../Plots/cosine_similarity_time.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"../Plots/cosine_similarity_time.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_clusters_0875 = (\n",
    "    df.groupby(\"cluster_0.875\")[\"language\"].apply(set).reset_index()\n",
    ")\n",
    "multilingual_clusters_0875[\"num_lang\"] = multilingual_clusters_0875[\"language\"].apply(\n",
    "    len\n",
    ")\n",
    "multilingual_clusters_0875 = multilingual_clusters_0875[\n",
    "    multilingual_clusters_0875[\"num_lang\"] > 1\n",
    "]\n",
    "df[\"is_multilingual\"] = df[\"cluster_0.875\"].isin(\n",
    "    multilingual_clusters_0875[\"cluster_0.875\"]\n",
    ")\n",
    "\n",
    "\n",
    "def get_range(x):\n",
    "    \"\"\"\n",
    "    input: groupby object in pandas.\n",
    "    output: output difference between min and max (without NAs) within each group.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = x.dropna()\n",
    "        return x.max() - x.min()\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "df[\"claim_date\"] = pd.to_datetime(df[\"claim_date\"], utc=True, errors=\"coerce\")\n",
    "a = (\n",
    "    df.groupby(\"cluster_0.875\")[\"claim_date\"]\n",
    "    .apply(get_range)\n",
    "    .apply(lambda x: float(x.days) if not pd.isnull(x) else x)\n",
    ")\n",
    "a = a.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "legend_fontsize = 20\n",
    "title_fontsize = 24\n",
    "label_fontsize = 20\n",
    "tick_fontsize = 16\n",
    "\n",
    "max_days = 28\n",
    "\n",
    "multilingual = df[\n",
    "    (df[\"is_multilingual\"])\n",
    "    & (df[\"cluster_0.875\"] > 0.1)\n",
    "    & ~(df[\"timedifference\"].isna())\n",
    "].timedifference // (24 * 60 * 60)\n",
    "monolingual = df[\n",
    "    ((~df[\"is_multilingual\"]) & (df[\"cluster_0.875\"] > 0.1))\n",
    "    & ~(df[\"timedifference\"].isna())\n",
    "].timedifference // (24 * 60 * 60)\n",
    "singleton = df[\n",
    "    (df[\"is_singleton_0.875\"]) & ~(df[\"timedifference\"].isna())\n",
    "].timedifference // (24 * 60 * 60)\n",
    "non_signleton = df[\n",
    "    (~df[\"is_singleton_0.875\"]) & ~(df[\"timedifference\"].isna())\n",
    "].timedifference // (24 * 60 * 60)\n",
    "\n",
    "df[\"timedifference_days\"] = df[\"timedifference\"] // (24 * 60 * 60)\n",
    "\n",
    "data_singleton = [singleton, non_signleton]\n",
    "labels_singleton = [\"Singleton\", \"Non-Singleton\"]\n",
    "data_language = [monolingual, multilingual]\n",
    "labels_language = [\"Monolingual\", \"Multilingual\"]\n",
    "\n",
    "colors_singleton = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "colors_language = [\"#ff7f0e\", \"#1f77b4\"]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "# gs = gridspec.GridSpec(2, 2, height_ratios=[0.66, 1], width_ratios=[1, 1])\n",
    "# gs = gridspec.GridSpec(2, 2, height_ratios=[0.66, 1], width_ratios=[1, 1], top=1)\n",
    "gs = gridspec.GridSpec(3, 2, height_ratios=[0.66, 0.005, 1], width_ratios=[1, 1])\n",
    "\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "cumulative_percentage = (\n",
    "    df[\"timedifference_days\"].value_counts(normalize=True).sort_index().cumsum() * 100\n",
    ")\n",
    "days = cumulative_percentage.index.to_numpy()\n",
    "percentages = cumulative_percentage.to_numpy()\n",
    "days = np.array(days, dtype=np.float64)\n",
    "percentages = np.array(percentages, dtype=np.float64)\n",
    "mask = np.isfinite(days)\n",
    "days = days[mask]\n",
    "percentages = percentages[mask]\n",
    "mask = days <= max_days\n",
    "days = days[mask]\n",
    "percentages = percentages[mask]\n",
    "ax0.plot(days, percentages, linewidth=2, color=\"#1f77b4\")\n",
    "ax0.scatter(days, percentages, color=\"#1f77b4\", marker=\"o\", s=40)\n",
    "ax0.fill_between(days, percentages, alpha=0.2, color=\"#1f77b4\")\n",
    "ax0.set_xlabel(\"Number of Days Since Claim Emergence\", fontsize=label_fontsize)\n",
    "ax0.set_ylabel(\"Cumulative Percentage of Claims\", fontsize=label_fontsize)\n",
    "\n",
    "xticks = np.arange(0, max_days + 1, max(1, max_days // 10))\n",
    "ax0.set_xticks(xticks)\n",
    "ax0.set_xticklabels(xticks, fontsize=tick_fontsize)\n",
    "yticks = np.arange(30, 101, 10)\n",
    "ax0.set_yticks(yticks)\n",
    "ax0.set_yticklabels([f\"{tick}%\" for tick in yticks], fontsize=label_fontsize)\n",
    "# ax0.set_title('Time Lag Between Claim Emergence and Fact-Checking', fontsize=title_fontsize+3)\n",
    "ax0.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "ax0.set_ylim(30, 100)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[2, 0])\n",
    "for d, label, color in zip(data_singleton, labels_singleton, colors_singleton):\n",
    "    cumulative_percentage = d.value_counts(normalize=True).sort_index().cumsum() * 100\n",
    "    days = cumulative_percentage.index.to_numpy()\n",
    "    percentages = cumulative_percentage.to_numpy()\n",
    "    days = np.array(days, dtype=np.float64)\n",
    "    percentages = np.array(percentages, dtype=np.float64)\n",
    "    mask = np.isfinite(days)\n",
    "    days = days[mask]\n",
    "    percentages = percentages[mask]\n",
    "    mask = days <= max_days\n",
    "    days_masked = days[mask]\n",
    "    percentages_masked = percentages[mask]\n",
    "    ax1.plot(days_masked, percentages_masked, linewidth=2, label=label, color=color)\n",
    "    ax1.scatter(days_masked, percentages_masked, marker=\"o\", s=40, color=color)\n",
    "\n",
    "for i in range(len(data_singleton) - 1):\n",
    "    d1 = data_singleton[i].value_counts(normalize=True).sort_index().cumsum() * 100\n",
    "    d2 = data_singleton[i + 1].value_counts(normalize=True).sort_index().cumsum() * 100\n",
    "    days1 = d1.index.to_numpy()\n",
    "    days2 = d2.index.to_numpy()\n",
    "    mask1 = np.isfinite(days1) & (days1 <= max_days)\n",
    "    mask2 = np.isfinite(days2) & (days2 <= max_days)\n",
    "    ax1.fill_between(\n",
    "        days1[mask1], np.array(d1[mask1]), np.array(d2[mask2]), alpha=0.2, color=\"red\"\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Number of Days Since Claim Emergence\", fontsize=label_fontsize)\n",
    "ax1.set_ylabel(\"Cumulative Percentage of Claims\", fontsize=label_fontsize)\n",
    "\n",
    "xticks = np.arange(0, max_days + 1, 7)\n",
    "ax1.set_xticks(xticks)\n",
    "ax1.set_xticklabels(xticks, fontsize=tick_fontsize)\n",
    "yticks = np.arange(30, 101, 10)\n",
    "ax1.set_yticks(yticks)\n",
    "ax1.set_yticklabels([f\"{tick}%\" for tick in yticks], fontsize=tick_fontsize)\n",
    "# ax1.set_title('Singleton vs. Non-Singleton', fontsize=title_fontsize)\n",
    "ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "ax1.legend(fontsize=legend_fontsize)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[2, 1])\n",
    "for d, label, color in zip(data_language, labels_language, colors_language):\n",
    "    cumulative_percentage = d.value_counts(normalize=True).sort_index().cumsum() * 100\n",
    "    days = cumulative_percentage.index.to_numpy()\n",
    "    percentages = cumulative_percentage.to_numpy()\n",
    "    days = np.array(days, dtype=np.float64)\n",
    "    percentages = np.array(percentages, dtype=np.float64)\n",
    "    mask = np.isfinite(days)\n",
    "    days = days[mask]\n",
    "    percentages = percentages[mask]\n",
    "    mask = days <= max_days\n",
    "    days_masked = days[mask]\n",
    "    percentages_masked = percentages[mask]\n",
    "    ax2.plot(days_masked, percentages_masked, linewidth=2, label=label, color=color)\n",
    "    ax2.scatter(days_masked, percentages_masked, marker=\"o\", s=40, color=color)\n",
    "\n",
    "for i in range(len(data_language) - 1):\n",
    "    d1 = data_language[i].value_counts(normalize=True).sort_index().cumsum() * 100\n",
    "    d2 = data_language[i + 1].value_counts(normalize=True).sort_index().cumsum() * 100\n",
    "    days1 = d1.index.to_numpy()\n",
    "    days2 = d2.index.to_numpy()\n",
    "    mask1 = np.isfinite(days1) & (days1 <= max_days)\n",
    "    mask2 = np.isfinite(days2) & (days2 <= max_days)\n",
    "    ax2.fill_between(\n",
    "        days1[mask1], np.array(d1[mask1]), np.array(d2[mask2]), alpha=0.2, color=\"red\"\n",
    "    )\n",
    "ax2.set_xlabel(\"Number of Days Since Claim Emergence\", fontsize=label_fontsize)\n",
    "ax2.set_ylabel(\"Cumulative Percentage of Claims\", fontsize=label_fontsize)\n",
    "\n",
    "xticks = np.arange(0, max_days + 1, 7)\n",
    "ax2.set_xticks(xticks)\n",
    "ax2.set_xticklabels(xticks, fontsize=tick_fontsize)\n",
    "yticks = np.arange(30, 101, 10)\n",
    "ax2.set_yticks(yticks)\n",
    "ax2.set_yticklabels([f\"{tick}%\" for tick in yticks], fontsize=tick_fontsize)\n",
    "# ax2.set_title('Monolingual vs. Multilingual', fontsize=title_fontsize)\n",
    "ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "ax2.legend(fontsize=legend_fontsize)\n",
    "\n",
    "# Add Labels to the Panels (A / B / C) - bold\n",
    "ax0.text(\n",
    "    -0.05,\n",
    "    1.175,\n",
    "    \"A\",\n",
    "    transform=ax0.transAxes,\n",
    "    fontsize=26,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "ax1.text(\n",
    "    -0.1,\n",
    "    1.1,\n",
    "    \"B\",\n",
    "    transform=ax1.transAxes,\n",
    "    fontsize=26,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "ax2.text(\n",
    "    -0.1,\n",
    "    1.1,\n",
    "    \"C\",\n",
    "    transform=ax2.transAxes,\n",
    "    fontsize=26,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "# Add space between rows\n",
    "plt.savefig(\"../Plots/cumulative_percentage_claims.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"../Plots/cumulative_percentage_claims.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_dissimilar(cluster, threshhold):\n",
    "    \"\"\"\n",
    "    Find the most dissimilar claims in a cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cluster : int\n",
    "        The cluster id.\n",
    "    threshhold : float\n",
    "        The cosine similarity threshhold.\n",
    "    \"\"\"\n",
    "    # Get all claims in the cluster\n",
    "    claims_in_cluster = df[\n",
    "        df[f\"cluster_{threshhold}\"] == cluster\n",
    "    ].claim_minimal.to_list()\n",
    "    # Find two most dissimilar nodes\n",
    "    embeddings_cluster = [embeddings[x] for x in claims_in_cluster]\n",
    "    # Print hash of embeddings\n",
    "    cosine_similarities_cluster = cosine_similarity(embeddings_cluster)\n",
    "    np.fill_diagonal(cosine_similarities_cluster, np.inf)\n",
    "    # Get the indices of the most dissimilar vectors.\n",
    "    i, j = np.unravel_index(\n",
    "        cosine_similarities_cluster.argmin(), cosine_similarities_cluster.shape\n",
    "    )\n",
    "    return claims_in_cluster[i], claims_in_cluster[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {}\n",
    "\n",
    "for threshhold in thresholds:\n",
    "    # Get the edge list for the threshold\n",
    "    edge_list = edge_lists[threshhold]\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph(edge_list)\n",
    "\n",
    "    # Create a dict with all paths\n",
    "    paths[threshhold] = {}\n",
    "    for cluster in tqdm(\n",
    "        df[f\"cluster_{threshhold}\"].unique(),\n",
    "        desc=f\"Calculating paths for threshold {threshhold}\",\n",
    "    ):\n",
    "        # Skip singletons\n",
    "        if cluster == 0:\n",
    "            continue\n",
    "\n",
    "        # Check if more than two claims in cluster\n",
    "        if (\n",
    "            df[df[f\"cluster_{threshhold}\"] == cluster].shape[0]\n",
    "            <= 2 | df[df[f\"cluster_{threshhold}\"] == cluster].shape[0]\n",
    "            > 50\n",
    "        ):\n",
    "            continue\n",
    "        # Find the most dissimilar claims in the cluster\n",
    "        from_node, to_node = find_most_dissimilar(cluster, threshhold)\n",
    "        # Calculate the path between the two claims\n",
    "        try:\n",
    "            path = nx.shortest_path(G, source=from_node, target=to_node)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Add the path to the dict\n",
    "        paths[threshhold][cluster] = {}\n",
    "        paths[threshhold][cluster][\"path\"] = path\n",
    "        paths[threshhold][cluster][\"origin\"] = from_node\n",
    "        paths[threshhold][cluster][\"destination\"] = to_node\n",
    "        paths[threshhold][cluster][\"cosine_similarity\"] = cosine_similarity(\n",
    "            [embeddings[from_node], embeddings[to_node]]\n",
    "        )[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = df.set_index(\"claim_minimal\").language.to_dict()\n",
    "\n",
    "for threshhold in paths.keys():\n",
    "    for cluster in paths[threshhold].keys():\n",
    "        paths[threshhold][cluster][\"origin_lang\"] = langs[\n",
    "            paths[threshhold][cluster][\"origin\"]\n",
    "        ]\n",
    "        paths[threshhold][cluster][\"destination_lang\"] = langs[\n",
    "            paths[threshhold][cluster][\"destination\"]\n",
    "        ]\n",
    "        paths[threshhold][cluster][\"path_lang\"] = [\n",
    "            langs[x] for x in paths[threshhold][cluster][\"path\"]\n",
    "        ]\n",
    "        paths[threshhold][cluster][\"length\"] = len(paths[threshhold][cluster][\"path\"])\n",
    "        paths[threshhold][cluster][\"number_of_language_switches\"] = sum(\n",
    "            [\n",
    "                1\n",
    "                for i in range(1, len(paths[threshhold][cluster][\"path_lang\"]))\n",
    "                if paths[threshhold][cluster][\"path_lang\"][i]\n",
    "                != paths[threshhold][cluster][\"path_lang\"][i - 1]\n",
    "            ]\n",
    "        )\n",
    "        # Number of unique languages\n",
    "        paths[threshhold][cluster][\"number_of_unique_languages\"] = len(\n",
    "            set(paths[threshhold][cluster][\"path_lang\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (\n",
    "    pd.DataFrame(paths[0.875])\n",
    "    .T.groupby(\"number_of_language_switches\")[\"cosine_similarity\"]\n",
    "    .agg([\"mean\", \"count\", \"std\"])\n",
    "    .dropna()\n",
    ")\n",
    "stats[\"std\"] = stats[\"std\"] / np.sqrt(stats[\"count\"])\n",
    "y_change = {0.825: 0.06, 0.8: 0.015, 0.775: -0.015, 0.75: -0.06}\n",
    "\n",
    "# Two subplots in two rows\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "for threshold in paths.keys():\n",
    "    stats = (\n",
    "        pd.DataFrame(paths[threshold])\n",
    "        .T.groupby(\"length\")[\"cosine_similarity\"]\n",
    "        .agg([\"mean\", \"count\", \"std\"])\n",
    "    )\n",
    "    stats = stats[stats.index <= 8]\n",
    "    stats[\"std\"] = stats[\"std\"] / np.sqrt(stats[\"count\"])\n",
    "    stats = stats[stats[\"count\"] > 10]\n",
    "    stats[\"mean\"] = stats[\"mean\"].astype(float)\n",
    "    stats[\"std\"] = stats[\"std\"].astype(float)\n",
    "    ax1.plot(\n",
    "        stats.index, stats[\"mean\"], \"o-\", markersize=2, label=f\"Threshold {threshold}\"\n",
    "    )\n",
    "    ax1.set_xlim(-0.5, 8.5)\n",
    "    ax1.set_ylim(0.3, 1)\n",
    "    ax1.grid(\n",
    "        True, which=\"both\", axis=\"both\", color=\"grey\", linestyle=\":\", linewidth=0.5\n",
    "    )\n",
    "    # ax1.set_xticklabels([])\n",
    "    end = stats.index[-1]\n",
    "    height = stats[\"mean\"].iloc[-1]\n",
    "    ax1.set_ylabel(\"Cosine Similarity\", fontsize=14)\n",
    "    ax1.set_xlabel(\"Length of Shortest Path\", fontsize=14)\n",
    "    ax1.text(\n",
    "        end,\n",
    "        height + y_change.get(threshold, 0),\n",
    "        f\"Threshold {threshold}\",\n",
    "        fontsize=10,\n",
    "        verticalalignment=\"center\",\n",
    "        bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "        color=ax1.lines[-1].get_color(),\n",
    "    )\n",
    "\n",
    "\n",
    "y_change_2 = {0.875: 0.03, 0.85: 0.0, 0.825: -0.03, 0.775: -0.06, 0.75: -0.03}\n",
    "for threshold in paths.keys():\n",
    "    stats = (\n",
    "        pd.DataFrame(paths[threshold])\n",
    "        .T.groupby(\"number_of_language_switches\")[\"cosine_similarity\"]\n",
    "        .agg([\"mean\", \"count\", \"std\"])\n",
    "    )\n",
    "    stats[\"std\"] = stats[\"std\"] / np.sqrt(stats[\"count\"])\n",
    "    stats = stats[stats[\"count\"] > 10]\n",
    "    stats[\"mean\"] = stats[\"mean\"].astype(float)\n",
    "    stats[\"std\"] = stats[\"std\"].astype(float)\n",
    "    ax2.plot(\n",
    "        stats.index, stats[\"mean\"], \"o-\", markersize=2, label=f\"Threshold {threshold}\"\n",
    "    )\n",
    "    ax2.set_xlim(-0.5, 8.5)\n",
    "    ax2.set_ylim(0.3, 1)\n",
    "    end = stats.index[-1]\n",
    "    height = stats[\"mean\"].iloc[-1]\n",
    "    ax2.set_ylabel(\"Cosine Similarity\", fontsize=14)\n",
    "    ax2.set_xlabel(\"Number of Language Switches\", fontsize=14)\n",
    "    ax2.text(\n",
    "        end,\n",
    "        height + y_change_2.get(threshold, 0),\n",
    "        f\"Threshold {threshold}\",\n",
    "        fontsize=10,\n",
    "        verticalalignment=\"center\",\n",
    "        bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=0.5, ec=\"k\"),\n",
    "        color=ax2.lines[-1].get_color(),\n",
    "    )\n",
    "\n",
    "# ax2.legend()\n",
    "# Reduce horizontal space between plots\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "\n",
    "# Add Labels to the Panels (A / B) - bold\n",
    "ax1.text(\n",
    "    -0.075,\n",
    "    1.07,\n",
    "    \"A\",\n",
    "    transform=ax1.transAxes,\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "ax2.text(\n",
    "    -0.075,\n",
    "    1.07,\n",
    "    \"B\",\n",
    "    transform=ax2.transAxes,\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    va=\"top\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "plt.savefig(\n",
    "    \"../Plots/average_similarity_by_lengthandlanguages.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.savefig(\n",
    "    \"../Plots/average_similarity_by_lengthandlanguages.pdf\", bbox_inches=\"tight\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
