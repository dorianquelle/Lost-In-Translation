{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Download necessary nltk resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find word that are most associated with mutlilingual clusters\n",
    "df = pd.read_csv(\"../Data/df_with_clusters_local_translated_with_clusters.csv\")\n",
    "# get all cluster_0.875 that contains >1 language\n",
    "multi_cluster = df.groupby(\"cluster_0.875\").apply(lambda x: len(set(x.language)) > 1)\n",
    "multilingual_clusters = multi_cluster[multi_cluster].index\n",
    "df[\"is_multilingual\"] = df[\"cluster_0.875\"].isin(multilingual_clusters)\n",
    "df[\"is_singleton\"] = df[\"cluster_0.875\"] == 0\n",
    "df = df[~df.tranlated_claimReviewed.isna()]\n",
    "\n",
    "# Function to convert nltk POS tags to first character used by WordNetLemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    return None\n",
    "\n",
    "# Initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to process text\n",
    "def process_text(text):\n",
    "    try:\n",
    "        # Step 1: Cast to lower\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Step 2: Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Step 3: Remove non-noun words\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        nouns = [word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "        \n",
    "        # Step 4: Lemmatize\n",
    "        #lemmatized_nouns = [lemmatizer.lemmatize(noun, get_wordnet_pos('N')) for noun in nouns]\n",
    "        lemmatized_nouns = [lemmatizer.lemmatize(noun) for noun in nouns]\n",
    "        \n",
    "        # remove words with length < 3\n",
    "        lemmatized_nouns = [word for word in lemmatized_nouns if len(word) > 2]\n",
    "        \n",
    "\n",
    "        return ' '.join(lemmatized_nouns)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'Translation' column\n",
    "df['Processed_Translation'] = df['tranlated_claimReviewed'].progress_apply(process_text)\n",
    "df[\"tokens\"] = df[\"Processed_Translation\"].apply(lambda x: set(x.split(\" \")) if isinstance(x,str) else set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = {}\n",
    "all_tokens = set.union(*df[\"tokens\"])\n",
    "\n",
    "singleton_tokens = {True:{},False:{}}\n",
    "for i,r in tqdm(df.iterrows()):\n",
    "    for token in r[\"tokens\"]:\n",
    "        if token in singleton_tokens[r[\"is_singleton\"]]:\n",
    "            singleton_tokens[r[\"is_singleton\"]][token] += 1\n",
    "            if token not in singleton_tokens[not r[\"is_singleton\"]]:\n",
    "                singleton_tokens[not r[\"is_singleton\"]][token] = 1\n",
    "        else:\n",
    "            singleton_tokens[r[\"is_singleton\"]][token] = 1\n",
    "            if token not in singleton_tokens[not r[\"is_singleton\"]]:\n",
    "                singleton_tokens[not r[\"is_singleton\"]][token] = 1\n",
    "\n",
    "# Filter singleton_tokens True & False for at least 50 occurences\n",
    "singleton_tokens[True] = {k:v/sum(list(singleton_tokens[True].values())) for k,v in singleton_tokens[True].items() if v > 50}\n",
    "singleton_tokens[False] = {k:v/sum(list(singleton_tokens[False].values())) for k,v in singleton_tokens[False].items() if v > 50}\n",
    "relative = {}\n",
    "for token in set(singleton_tokens[True].keys()):\n",
    "    if token in singleton_tokens[False]:\n",
    "        relative[token] = singleton_tokens[True][token] / (singleton_tokens[False][token])\n",
    "\n",
    "most_single, least_single = [], []\n",
    "# Print TOP 10 MOST SINGLTO\n",
    "print(\"TOP 10 MOST SINGLTON\")\n",
    "for i,(k,v) in enumerate(sorted(relative.items(),key=lambda x: x[1],reverse=True)[:10]):\n",
    "    print(i, \": \", k,f\"{v:.2f}\")\n",
    "    most_single.append(f\"{k} ({1/v:.2f})\")\n",
    "\n",
    "# Print TOP 10 LEAST SINGLTON\n",
    "print(\"TOP 10 LEAST SINGLTON\")\n",
    "for i,(k,v) in enumerate(sorted(relative.items(),key=lambda x: x[1],reverse=False)[:10]):\n",
    "    print(i, \": \", k,f\"{v:.2f}\")\n",
    "    least_single.append(f\"{k} ({1/v:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_tokens = {True:{},False:{}}\n",
    "for i,r in tqdm(df.loc[~df[\"is_singleton\"]].iterrows()):\n",
    "    for token in r[\"tokens\"]:\n",
    "        if token in singleton_tokens[r[\"is_multilingual\"]]:\n",
    "            singleton_tokens[r[\"is_multilingual\"]][token] +=1\n",
    "            if token not in singleton_tokens[not r[\"is_multilingual\"]]:\n",
    "                singleton_tokens[not r[\"is_multilingual\"]][token] = 1\n",
    "        else:\n",
    "            singleton_tokens[r[\"is_multilingual\"]][token] = 1\n",
    "            if token not in singleton_tokens[not r[\"is_multilingual\"]]:\n",
    "                singleton_tokens[not r[\"is_multilingual\"]][token] = 1\n",
    "\n",
    "# Filter singleton_tokens True & False for at least 50 occurences\n",
    "singleton_tokens[True] = {k:v/sum(list(singleton_tokens[True].values())) for k,v in singleton_tokens[True].items() if v > 50}\n",
    "singleton_tokens[False] = {k:v/sum(list(singleton_tokens[False].values())) for k,v in singleton_tokens[False].items() if v > 50}\n",
    "relative = {}\n",
    "for token in set(singleton_tokens[True].keys()):\n",
    "    if token in singleton_tokens[False]:\n",
    "        relative[token] = singleton_tokens[True][token] / (singleton_tokens[False][token])\n",
    "\n",
    "most_multilingual, least_multilingual = [], []\n",
    "# Print TOP 10 MOST SINGLTO\n",
    "print(\"TOP 10 MOST SINGLTON\")\n",
    "for i,(k,v) in enumerate(sorted(relative.items(),key=lambda x: x[1],reverse=True)[:10]):\n",
    "    print(i, \": \", k,f\"{v:.2f}\")\n",
    "    most_multilingual.append(f\"{k} ({v:.2f})\")\n",
    "\n",
    "# Print TOP 10 LEAST SINGLTON\n",
    "print(\"TOP 10 LEAST SINGLTON\")\n",
    "for i,(k,v) in enumerate(sorted(relative.items(),key=lambda x: x[1],reverse=False)[:10]):\n",
    "    print(i, \": \", k,f\"{v:.2f}\")\n",
    "    least_multilingual.append(f\"{k} ({v:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'Translation' column\n",
    "clustered = df[~(df[\"is_singleton\"]) & ~(df[\"datePublished\"].isna())]\n",
    "clustered[\"datePublished\"] = pd.to_datetime(clustered[\"datePublished\"], utc = True, errors = \"coerce\")\n",
    "islonglasting = clustered.groupby(\"cluster_0.875\").apply(lambda x: (x.datePublished.max() - x.datePublished.min()).days > 30)\n",
    "longlasting = islonglasting[islonglasting].index\n",
    "clustered[\"is_longlasting\"] = clustered[\"cluster_0.875\"].isin(longlasting)\n",
    "\n",
    "clustered['Processed_Translation'] = clustered['tranlated_claimReviewed'].progress_apply(process_text)\n",
    "clustered[\"tokens\"] = clustered[\"Processed_Translation\"].apply(lambda x: set(x.split(\" \")) if isinstance(x,str) else set())\n",
    "\n",
    "# Repeat for clustered and longlasting\n",
    "longlasting_tokens = {True:{},False:{}}\n",
    "for i,r in tqdm(clustered.iterrows()):\n",
    "    for token in r[\"tokens\"]:\n",
    "        if token in longlasting_tokens[r[\"is_longlasting\"]]:\n",
    "            longlasting_tokens[r[\"is_longlasting\"]][token] +=1\n",
    "            if token not in longlasting_tokens[not r[\"is_longlasting\"]]:\n",
    "                longlasting_tokens[not r[\"is_longlasting\"]][token] = 1\n",
    "        else:\n",
    "            longlasting_tokens[r[\"is_longlasting\"]][token] = 1\n",
    "            if token not in longlasting_tokens[not r[\"is_longlasting\"]]:\n",
    "                longlasting_tokens[not r[\"is_longlasting\"]][token] = 1\n",
    "\n",
    "# Filter singleton_tokens True & False for at least 50 occurences\n",
    "longlasting_tokens[True] = {k:v/sum(list(longlasting_tokens[True].values())) for k,v in longlasting_tokens[True].items() if v > 50}\n",
    "longlasting_tokens[False] = {k:v/sum(list(longlasting_tokens[False].values())) for k,v in longlasting_tokens[False].items() if v > 50}\n",
    "relative = {}\n",
    "for token in set(longlasting_tokens[True].keys()):\n",
    "    if token in longlasting_tokens[False]:\n",
    "        relative[token] = longlasting_tokens[True][token] / (longlasting_tokens[False][token])\n",
    "\n",
    "most_longlasting, least_longlasting = [], []\n",
    "# Print TOP 10 MOST SINGLTO\n",
    "print(\"TOP 10 MOST LONG LASTING\")\n",
    "for i,(k,v) in enumerate(sorted(relative.items(),key=lambda x: x[1],reverse=True)[:10]):\n",
    "    print(i, \": \", k,f\"{v:.2f}\")\n",
    "    most_longlasting.append(f\"{k} ({v:.2f})\")\n",
    "\n",
    "# Print TOP 10 LEAST SINGLTON\n",
    "print(\"TOP 10 LEAST LONG LASTING\")\n",
    "for i,(k,v) in enumerate(sorted(relative.items(),key=lambda x: x[1],reverse=False)[:10]):\n",
    "    print(i, \": \", k,f\"{v:.2f}\")\n",
    "    least_longlasting.append(f\"{k} ({v:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"most_single\":most_single,\"least_single\":least_single,\"most_multilingual\":most_multilingual,\"least_multilingual\":least_multilingual, \"most_longlasting\":most_longlasting,\"least_longlasting\":least_longlasting})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
