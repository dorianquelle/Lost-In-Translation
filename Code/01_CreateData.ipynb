{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bb7dd-a118-4f32-8afe-6be3d1346a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpcore\n",
    "setattr(httpcore, 'SyncHTTPTransport', any)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "from urllib.parse import urlparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import glob\n",
    "import requests\n",
    "import uuid\n",
    "from langdetect import detect, DetectorFactory\n",
    "import re\n",
    "from googletrans import Translator\n",
    "from tldextract import extract\n",
    "from langdetect import detect_langs\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import html\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(https?://(?:www\\.|)twitter\\.com/[\\w\\d_/]+|https?://t\\.co/[\\w\\d]+)'\n",
    "# Define a function to find matches and create a dictionary\n",
    "def find_twitter_links(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return {}\n",
    "    matches = re.findall(pattern, text)\n",
    "    # Create a dictionary where keys are the match order (1-indexed) and values are the matches\n",
    "    return {i+1: match for i, match in enumerate(matches)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FACTCHECKS\n",
    "link = \"https://storage.googleapis.com/datacommons-feeds/factcheck/latest/data.json\"\n",
    "# Download JSON with requests\n",
    "r = requests.get(link)\n",
    "# Load JSON\n",
    "factchecks = r.json()\n",
    "\n",
    "columns = [\"datePublished\",\"claimReviewed\",\"author\"]\n",
    "temp_df = pd.concat(map(pd.DataFrame,[x[\"item\"] for x in factchecks[\"dataFeedElement\"]]))\n",
    "temp_df = temp_df[columns]\n",
    "temp_df[\"name\"] = temp_df[\"author\"].apply(lambda x: x.get(\"name\",None) if isinstance(x,dict) else None)\n",
    "temp_df[\"url\"] = temp_df[\"author\"].apply(lambda x: x.get(\"url\",None) if isinstance(x,dict) else None)\n",
    "temp_df[\"twitter_urls\"] = temp_df[\"claimReviewed\"].apply(lambda x: find_twitter_links(x))\n",
    "temp_df[\"@type\"] = temp_df[\"author\"].apply(lambda x: x.get(\"@type\",None) if isinstance(x,dict) else None)\n",
    "temp_df.drop(columns=[\"author\"],inplace=True)\n",
    "temp_df = temp_df[~pd.isna(temp_df.claimReviewed)]\n",
    "temp_df[\"claimReviewed\"] = temp_df[\"claimReviewed\"].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "temp_df.reset_index(drop=True,inplace=True)\n",
    "google_dump = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FACTCHECKS\n",
    "link = \"https://storage.googleapis.com/datacommons-feeds/factcheck/latest/data.json\"\n",
    "# Download JSON with requests\n",
    "r = requests.get(link)\n",
    "# Load JSON\n",
    "factchecks = r.json()\n",
    "columns = [\"datePublished\",\"claimReviewed\",\"name\", \"verdict\",\"author_url\", \"url\",\"twitter_urls\"]\n",
    "\n",
    "# Prepare empty list to store dataframes\n",
    "dfs = []\n",
    "# Iterate over each DataFeedItem in dataFeedElement\n",
    "for data_item in tqdm(factchecks[\"dataFeedElement\"]):\n",
    "    # Create temporary DataFrame for each item\n",
    "    temp_df = pd.DataFrame(data_item[\"item\"])\n",
    "    \n",
    "    if 'author' in temp_df.columns and \"claimReviewed\" in temp_df.columns:\n",
    "        temp_df[\"name\"] = temp_df[\"author\"].apply(lambda x: x.get(\"name\",None) if isinstance(x,dict) else None)\n",
    "        temp_df[\"author_url\"] = temp_df[\"author\"].apply(lambda x: x.get(\"url\",None) if isinstance(x,dict) else None)\n",
    "        temp_df[\"@type\"] = temp_df[\"author\"].apply(lambda x: x.get(\"@type\",None) if isinstance(x,dict) else None)\n",
    "        temp_df[\"twitter_urls\"] = temp_df[\"claimReviewed\"].apply(lambda x: find_twitter_links(x))\n",
    "        if \"reviewRating\" in temp_df.columns:\n",
    "            temp_df[\"verdict\"] = temp_df[\"reviewRating\"].apply(lambda x: x[\"alternateName\"] if isinstance(x,dict) and \"alternateName\" in x else None)\n",
    "        else:\n",
    "            temp_df[\"verdict\"] = None\n",
    "        temp_df.drop(columns=[\"author\"],inplace=True)\n",
    "\n",
    "    temp_df[\"url\"] = data_item[\"url\"]  # Add URL from DataFeedItem to each row in the DataFrame\n",
    "    dfs.append(temp_df)  # Append temporary DataFrame to list\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Filter rows where claimReviewed is not NaN\n",
    "df = df[~pd.isna(df.claimReviewed)]\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "google_dump = df\n",
    "google_dump = google_dump[columns]\n",
    "google_dump[\"inLanguage\"] = None\n",
    "print(google_dump.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic:\n",
    "# If text is shorter than < 50 take it as claim else claim\n",
    "def decide(x):\n",
    "    \"\"\"\n",
    "    Decides which part of the Fact-Check is a claim. \n",
    "    Preference Order: Claim > Body > Headline\n",
    "    \"\"\"\n",
    "    \n",
    "    if x.claimReviewed:\n",
    "        return(x.claimReviewed)\n",
    "    else:\n",
    "        if x.headline and isinstance(x.headline,str):\n",
    "            if len(x.headline) <= average_length_claim_reviewd + 1.96*var_claim_reviewd**0.5:\n",
    "                return(x.headline)\n",
    "        if x.text and isinstance(x.text,str):\n",
    "            if len(x.text) <= average_length_claim_reviewd + 1.96*var_claim_reviewd**0.5:\n",
    "                return(x.text)\n",
    "        else:\n",
    "            return(float(\"nan\"))\n",
    "    return(float(\"nan\"))\n",
    "\n",
    "# Calulate the Average Length & Variance Based on the Google Dump\n",
    "average_length_claim_reviewd = google_dump.claimReviewed.apply(lambda x: len(x)).mean()\n",
    "var_claim_reviewd = google_dump.claimReviewed.apply(lambda x: len(x)).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d520556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_and_load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        file_contents = f.read()\n",
    "\n",
    "    # Assuming JSON objects are separated by line breaks for this example\n",
    "    potential_json_objects = file_contents.split('\\n')\n",
    "\n",
    "    valid_json_objects = []\n",
    "    for obj_str in potential_json_objects:\n",
    "        try:\n",
    "            json_obj = json.loads(obj_str)\n",
    "            valid_json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Found an invalid JSON object, skipping...\")\n",
    "\n",
    "    return valid_json_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "count = 0\n",
    "# Columns to extract\n",
    "columns = [\"datePublished\",\"claimReviewed\",\"name\",\"author_url\", \"url\",\"twitter_urls\",\"inLanguage\",\"verdict\", \"claim_date\",\"text\"]\n",
    "\n",
    "# Loop through all Dataframe\n",
    "for f in tqdm(os.listdir(\"../Data/Factcheck/\")):\n",
    "    if \".json\" in f:\n",
    "        if \"2024\" in f:\n",
    "            temp_json = fix_and_load_json(\"../Data/Factcheck/{}\".format(f))\n",
    "            temp_df = [x[\"_source\"] for x in temp_json if \"_source\" in x]\n",
    "            temp_df = pd.DataFrame(temp_df)\n",
    "\n",
    "            temp_df[\"twitter_urls\"] = temp_df[\"claim_review_body\"].apply(lambda x: find_twitter_links(x))\n",
    "            temp_df[\"claimReviewed\"] = temp_df[\"claim_review_headline\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "            temp_df[\"text\"] = temp_df[\"claim_review_body\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "            temp_df[\"headline\"] = temp_df[\"claim_review_headline\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "\n",
    "            temp_df[\"verdict\"] = temp_df[\"claim_review_result\"]\n",
    "            temp_df[\"author_url\"] = temp_df[\"author_link\"]\n",
    "            temp_df[\"name\"] = temp_df[\"author\"]\n",
    "            temp_df[\"url\"] = temp_df[\"claim_review_url\"]\n",
    "            temp_df[\"datePublished\"] = pd.to_datetime(temp_df[\"created_at\"], errors='coerce')\n",
    "            temp_df[\"inLanguage\"] = temp_df[\"language\"]\n",
    "            try:\n",
    "                raw_claim_review = temp_df[\"raw_claim_review\"].apply(lambda x: json.loads(x) if isinstance(x,str) else x)\n",
    "                temp_df[\"claim_date\"] = raw_claim_review.apply(lambda x: x.get(\"claimDate\",None) if isinstance(x,dict) else None)\n",
    "            except Exception as e:\n",
    "                print(f\"\\033[91mError in New: {e}\\033[0m\")\n",
    "                temp_df[\"claim_date\"] = None\n",
    "\n",
    "            temp_df[\"author\"] = temp_df[\"author\"].apply(lambda x: \", \".join(x) if isinstance(x,list) else str(x))\n",
    "            temp_df[\"author_link\"] = temp_df[\"author_link\"].apply(lambda x: \"\".join(x) if isinstance(x,str) else str(x))\n",
    "        else:\n",
    "            temp_json = pd.read_json(\"../Data/Factcheck/{}\".format(f))\n",
    "            temp_df = pd.DataFrame(temp_json)\n",
    "\n",
    "            temp_df[\"twitter_urls\"] = temp_df[\"raw\"].apply(lambda x: find_twitter_links(x[\"claim_review_body\"]))\n",
    "            temp_df[\"claimReviewed\"] = temp_df[\"claimReviewed\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "            temp_df[\"text\"] = temp_df[\"text\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "            temp_df[\"headline\"] = temp_df[\"headline\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "\n",
    "            temp_df[\"verdict\"] = temp_df[\"reviewRating\"].apply(lambda x: x.get(\"alternateName\",None) if isinstance(x,dict) else None)\n",
    "\n",
    "            if \"author\" in temp_df.columns:\n",
    "                temp_df[\"author_url\"] = temp_df.author.apply(lambda x: x.get(\"url\",None) if isinstance(x,dict) else None)\n",
    "                temp_df[\"name\"] = temp_df[\"author\"].apply(lambda x: x.get(\"name\",None) if isinstance(x,dict) else None)\n",
    "            else:\n",
    "                temp_df[\"author_url\"] = None\n",
    "                temp_df[\"name\"] = None\n",
    "\n",
    "            temp_df[\"url\"] = temp_df[\"url\"]\n",
    "            temp_df[\"datePublished\"] = pd.to_datetime(temp_df[\"datePublished\"], errors='coerce')\n",
    "            temp_df[\"inLanguage\"] = temp_df[\"inLanguage\"]\n",
    "            temp_df[\"raw_claim_review\"] = temp_df.apply(lambda x: json.loads(x.get(\"raw\",{}).get(\"raw_claim_review\",\"{}\")),axis=1)\n",
    "            temp_df[\"claim_date\"] = temp_df[\"raw_claim_review\"].apply(lambda x: x.get(\"claimDate\", None) if x else None)\n",
    "        \n",
    "        count += temp_df.shape[0]\n",
    "\n",
    "        # Extract which column is the claimReviewed\n",
    "        for i,r in temp_df.iterrows():\n",
    "            temp_df.loc[i,\"claimReviewed\"] = decide(r)\n",
    "        \n",
    "        temp_df = temp_df[columns]    \n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        all_dfs.append(temp_df)\n",
    "        \n",
    "scraped_factchecks = pd.concat(all_dfs)\n",
    "scraped_factchecks = scraped_factchecks.reset_index(drop = True)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2ff01",
   "metadata": {},
   "source": [
    "### Analysis of Duration to Publish Fact-check - We should do this when we have the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_factchecks[\"claim_date\"] = pd.to_datetime(scraped_factchecks[\"claim_date\"], errors='coerce', utc=True)\n",
    "scraped_factchecks[\"datePublished\"] = pd.to_datetime(scraped_factchecks[\"datePublished\"], errors='coerce', utc=True)\n",
    "scraped_factchecks[\"timedifference\"] = abs(scraped_factchecks[\"claim_date\"] - scraped_factchecks[\"datePublished\"]).apply(lambda x: x.total_seconds() if not pd.isna(x) else x)\n",
    "scraped_factchecks.loc[scraped_factchecks[\"timedifference\"] < 3600,\"timedifference\"] = None\n",
    "scraped_factchecks[\"timedifference_hours\"] = scraped_factchecks[\"timedifference\"] // 3600\n",
    "scraped_factchecks[\"timedifference_days\"] = scraped_factchecks[\"timedifference\"] // (3600 * 24)\n",
    "scraped_factchecks[\"author_url\"] = scraped_factchecks[\"author_url\"].apply(lambda x: \", \".join(x) if isinstance(x,list) else str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0181f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Ratings:\n",
    "# - Only look at Ratings that have been given at least 100 times\n",
    "# - remove all punctuation, case, and spaces\n",
    "scraped_factchecks[\"verdict\"] = scraped_factchecks[\"verdict\"].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "scraped_factchecks[\"verdict\"] = scraped_factchecks[\"verdict\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x) if isinstance(x,str) else x)\n",
    "all_verdicts = scraped_factchecks[\"verdict\"].value_counts()\n",
    "all_verdicts = all_verdicts[all_verdicts > 50]\n",
    "verdict_mapping = {\"false\": \"False\", \"falso\": \"False\", \"misleading\": \"Mostly False\", \n",
    "    \"ŸÖÿ∂ŸÑŸÑ\": \"Mostly False\", \"mostly false\": \"Mostly False\", \"half true\": \"Mostly True\", \n",
    "    \"yanlƒ±≈ü\": \"False\", \"ÿ≤ÿßÿ¶ŸÅ\": \"False\", \"true\": \"True\", \"ÿÆÿ∑ÿ£\": \"False\", \n",
    "    \"fake\": \"False\", \"ÈåØË™§\": \"False\", \"faux\": \"False\", \"misleading content\": \"Mostly False\",\n",
    "    \"enga√±oso\": \"Mostly False\", \"fa≈Çsz\": \"False\", \"verdadeiro\": \"True\",\n",
    "    \"false context\": \"False\", \"errado\": \"False\", \"yanlis\": \"False\", \n",
    "    \"salah misleading content\": \"Mostly False\", \"mentira\": \"False\", \n",
    "    \"mixture\": \"NA\", \"fabricated content\": \"False\", \"pants on fire\": \"False\", \n",
    "    \"partly false\": \"Mostly False\", \"incorrect\": \"False\", \"other\": \"NA\",\n",
    "    \"manipulated content\": \"False\", \"keliru\": \"False\", \"salah false context\": \"False\",\n",
    "    \"2 false context\": \"False\", \"falsch\": \"False\", \"missing context\": \"Mostly False\", \n",
    "    \"notizia falsa\": \"False\", \"2 misleading content\": \"Mostly False\", \n",
    "    \"predominantemente falso\": \"False\", \"prawda\": \"True\", \"salah\": \"False\", \n",
    "    \"enganoso\": \"Mostly False\", \"œÄŒ±œÅŒ±œÄŒªŒ∑œÅŒøœÜœåœÅŒ∑œÉŒ∑\": \"Mostly False\", \"fals\": \"False\", \n",
    "    \"verdadeiro mas\": \"Mostly True\", \"fact crescendo rating false\": \"False\",\n",
    "    \"mostly true\": \"Mostly True\", \"yanli≈ü\": \"False\", \"verdadero\": \"True\", \n",
    "    \"salah fabricated content\": \"False\", \"satire\": \"NA\", \"doƒüru\": \"True\", \n",
    "    \"unproven\": \"NA\", \"ÈÉ®ÂàÜÈåØË™§\": \"Mostly False\", \"sesat\": \"False\", \n",
    "    \"impostor content\": \"False\", \"fuori contesto\": \"Mostly False\", \"nepravda\": \"False\", \n",
    "    \"‰∏çÂØ¶\": \"False\", \"2 manipulated content\": \"False\", \"salah manipulated content\": \"False\",\n",
    "    \"2 fabricated content\": \"False\", \"engany√≥s\": \"Mostly False\", \"œàŒµœÖŒ¥Œ≠œÇ\": \"False\", \n",
    "    \"salah impostor content\": \"False\", \"impreciso\": \"Mostly False\", \"inconclusive\": \"NA\",\n",
    "    \"pimenta na l√≠ngua\": \"NA\", \"ÿµÿ≠Ÿäÿ≠\": \"True\", \"falsk\": \"False\", \"netaƒçno\": \"False\",\n",
    "    \"kƒ±smen yanlƒ±≈ü\": \"Mostly False\", \"trompeur\": \"Mostly False\", \"manipulacja\": \"False\",\n",
    "    \"correct\": \"True\", \"n√¨\": \"NA\", \"ÿ•ÿ´ÿßÿ±ÿ©\": \"NA\", \"miscaptioned\": \"False\", \n",
    "    \"correct attribution\": \"True\", \"clarification\": \"NA\", \"cuestionable\": \"NA\", \n",
    "    \"inaccurate\": \"Mostly False\", \"altered image\": \"False\", \"cest faux\": \"False\",\n",
    "    \"misplaced context\": \"Mostly False\", \"vero\": \"True\", \"helt feil\": \"False\", \n",
    "    \"sin registro\": \"NA\", \"ÿ≥ÿßÿÆÿ±\": \"NA\", \"altered\": \"False\", \"pinocchio andante\": \"False\",\n",
    "    \"tak benar\": \"False\", \"‰∫ãÂØ¶ÈáêÊ∏Ö\": \"NA\", \"karma\": \"NA\", \"enganador\": \"Mostly False\", \n",
    "    \"meiaverdade\": \"NA\", \"ceri quasi\": \"NA\", \"irref√ºhrend\": \"Mostly False\",\n",
    "    \"no evidence\": \"NA\", \"distorcido\": \"Mostly False\", \"ŸÖÿ±ŸÉÿ®\": \"False\", \n",
    "    \"checked\": \"NA\", \"four pinocchios\": \"False\", \"false  content that has no basis in fact\": \"False\", \n",
    "    \"needs context\": \"NA\", \"fabricated\": \"False\", \"false connection\": \"False\",\n",
    "    \"ÿπŸÜŸàÿßŸÜ ŸÖÿ∂ŸÑŸÑ\": \"False\", \"raczej fa≈Çsz\": \"Mostly False\", \"ikke dokumenteret\": \"NA\", \n",
    "    \"tvrdnja je netoƒçna\": \"False\", \"rren√´\": \"False\", \"⁄ØŸÖÿ±ÿß€Å ⁄©ŸÜ ÿØÿπŸà€å\": \"NA\", \n",
    "    \"exagerado\": \"NA\", \"—Ñ–µ–π–∫\": \"False\", \"neistina\": \"False\", \"falta contexto\": \"Mostly False\", \n",
    "    \"unsupported\": \"NA\", \"–ø—Ä–∞–≤–¥–∞\": \"True\", \"ŸÅÿ±ÿ∂€å ÿØÿπŸà€å\": \"NA\", \"–º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è\": \"False\",\n",
    "    \"ŸÉÿ∞ÿ®\": \"False\", \"œàŒµœÖŒ¥ŒÆœÇ ŒπœÉœáœÖœÅŒπœÉŒºœåœÇ\": \"False\", \"ÿßŸÜÿ™ŸÇÿßÿ¶Ÿä\": \"Mostly False\", \n",
    "    \"suspicious\": \"NA\", \"manipulated\": \"False\", \"ŸÖÿ±ŸÉÿ®ÿ©\": \"False\", \"5 clarification\": \"NA\", \n",
    "    \"scam\": \"False\", \"mostly correct\": \"Mostly True\", \"p√≥≈Çprawda\": \"Mostly True\", \n",
    "    \"chequeo m√∫ltiple\": \"NA\", \"sem contexto\": \"Mostly False\", \"ÿ™ÿ∂ŸÑŸäŸÑ\": \"Mostly False\",\n",
    "    \"‡§≠‡§∞‡§Æ‡§ï\": \"NA\", \"distorts the facts\": \"Mostly False\", \"salah satire\": \"False\", \"3\": \"NA\", \n",
    "    \"three pinocchios\": \"Mostly False\", \"verdadero pero\": \"Mostly True\", \n",
    "    \"commotion\": \"NA\", \"parcialmente falso\": \"Mostly False\", \"explainer\": \"NA\",\n",
    "    \"verdadeiro em partes\": \"Mostly True\", \"nieweryfikowalne\": \"NA\", \n",
    "    \"false  the primary claims of the content are factually inaccurate\": \"False\",\n",
    "    \"mix\": \"NA\", \"hard to categorise\": \"NA\", \"notizia vera\": \"True\", \n",
    "    \"real\": \"True\", \"false and misleading\": \"False\", \"selective\": \"Mostly False\", \n",
    "    \"ŸÖÿ¥ŸÉŸàŸÉ ŸÅŸäŸá\": \"NA\", \"–ª–æ–∂—å\": \"False\", \"misleading and false\": \"False\", \n",
    "    \"ÿ≤ÿßÿ¶ŸÅ ÿ¨ÿ≤ÿ¶Ÿäÿß\": \"Mostly False\", \"œàŒµœÖŒ¥ŒøŒµœÄŒπœÉœÑŒÆŒºŒ∑\": \"NA\", \"manipulated media\": \"False\",\n",
    "    \"partially true\": \"Mostly True\", \"misvisende\": \"Mostly False\", \"rren√´ e krypt√´\": \"False\", \n",
    "    \"verificamos\": \"NA\", \"verdad a medias\": \"Mostly True\", \n",
    "    \"ŒªŒµŒØœÄŒµŒπ Œ∏ŒµŒºŒ±œÑŒπŒ∫œå œÄŒµœÅŒπŒµœáœåŒºŒµŒΩŒø\": \"NA\", \"hetimet vazhdojn√´\": \"NA\",\n",
    "    \"two pinocchios\": \"Mostly False\", \"ŸÜÿßÿØÿ±ÿ≥ÿ™\": \"False\", \"descontextualizado\": \"Mostly False\", \n",
    "    \"sem registro\": \"NA\", \"misattributed\": \"False\", \"labeled satire\": \"NA\", \n",
    "    \"◊ú◊ê ◊†◊õ◊ï◊ü\": \"False\", \"2 false connection\": \"False\", \"hoax\": \"False\", \n",
    "    \"falsk men\": \"Mostly False\", \"false headline\": \"False\", \"partiellement faux\": \"Mostly False\", \n",
    "    \"panzana pazzesca\": \"False\", \"5050\": \"NA\", \"outdated\": \"NA\", \n",
    "    \"montaje\": \"False\", \"ÿ∫Ÿäÿ± ÿµÿ≠Ÿäÿ≠\": \"False\", \"immagine modificata\": \"False\", \n",
    "    \"ÿÆÿ±ÿßŸÅÿ©\": \"False\", \"false claim\": \"False\", \"flipflop\": \"NA\", \"–æ—á–µ–Ω—å —Å–ø–æ—Ä–Ω–æ\": \"NA\", \n",
    "    \"not the whole story\": \"Mostly False\", \"certo\": \"True\", \"1 true\": \"True\",\n",
    "    \"falsa\": \"False\", \"mostly true  mostly accurate but there is a minor error or problem\": \"Mostly True\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80226f",
   "metadata": {},
   "source": [
    "## Extract Twitter Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09494b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_scraped_dict = scraped_factchecks.set_index(\"url\")[\"twitter_urls\"]\n",
    "url_scraped_dict = {k:v for k,v in url_scraped_dict.items() if len(v) > 0}\n",
    "url_scraped_dict\n",
    "all_urls = [list(value.values()) for key, value in url_scraped_dict.items()]\n",
    "# Flatten the list\n",
    "all_urls = [item for sublist in all_urls for item in sublist]\n",
    "# Remove duplicates\n",
    "all_urls = list(set(all_urls))\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))\n",
    "def get_final_url(url):\n",
    "    # Check if url is a t.co link\n",
    "    if \"t.co\" in url:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            return url, response.url\n",
    "        except (requests.exceptions.RequestException, UnicodeDecodeError):\n",
    "            return url, url\n",
    "    else:\n",
    "        return url, url\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    url_mapping = dict(tqdm(executor.map(get_final_url, all_urls), total=len(all_urls)))\n",
    "\n",
    "\n",
    "url_scraped_dict_resolved = {}\n",
    "for key in tqdm(url_scraped_dict):\n",
    "    for number, link in url_scraped_dict[key].items():\n",
    "        if key not in url_scraped_dict_resolved:\n",
    "            url_scraped_dict_resolved[key] = {}\n",
    "        if \"t.co\" in link:\n",
    "            if link in url_mapping and url_mapping[link] != link and \"t.co\" not in url_mapping[link]:\n",
    "                to_add = {\"Resolved\":url_mapping[link], \"Original\":link}\n",
    "            else:\n",
    "                to_add = {\"Resolved\":link, \"Original\":link}\n",
    "            url_scraped_dict_resolved[key][number] = to_add\n",
    "        else:\n",
    "            url_scraped_dict_resolved[key][number] = {\"Resolved\":link, \"Original\":link}\n",
    "\n",
    "# First, convert the nested dictionary into a flat list of dictionaries\n",
    "rows_list = []\n",
    "for article_url, urls in url_scraped_dict_resolved.items():\n",
    "    for idx, url_dict in urls.items():\n",
    "        row_dict = {\n",
    "            'Article_Url': article_url,\n",
    "            'Index': idx,\n",
    "            'Original': url_dict['Original'],\n",
    "            'Resolved': url_dict['Resolved']\n",
    "        }\n",
    "        rows_list.append(row_dict)\n",
    "\n",
    "# Now, convert the list of dictionaries into a DataFrame\n",
    "url_df = pd.DataFrame(rows_list)\n",
    "url_df[\"Twitter_Link\"] = url_df[\"Resolved\"].apply(lambda x: True if \"twitter.com\" in x else False)\n",
    "url_df[\"Twitter_Id\"] = url_df[\"Resolved\"].apply(lambda x: x.split(\"/\")[-1] if \"twitter.com\" in x and \"status\" in x else None)\n",
    "\n",
    "# For each link set variable is_first to True if it is the first link with an associated Twitter ID in the article\n",
    "url_df = url_df[(url_df.Twitter_Id.notnull()) & (url_df.Twitter_Id.apply(lambda x: len(x) == 19 if isinstance(x,str) else False))]\n",
    "first_urls = url_df.groupby(\"Article_Url\").apply(lambda x: x[\"Index\"] == x[\"Index\"].min())\n",
    "first_urls = first_urls.reset_index(level=1, drop=True).to_dict()\n",
    "first_urls = {k for k,v in first_urls.items() if v}\n",
    "url_df = url_df[url_df.Article_Url.isin(first_urls)]\n",
    "# Export to CSV\n",
    "url_df.to_csv(\"../Data/URLs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86063281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geturl(x):\n",
    "    return extract(x).domain + \".\" + extract(x).suffix\n",
    "\n",
    "# Add inLanguage to Google Dump\n",
    "google_dump[\"inLanguage\"] = None\n",
    "\n",
    "# Merge Google Dump with other Factchecks\n",
    "df = pd.concat([scraped_factchecks,google_dump])\n",
    "\n",
    "# Remove all Rows with no associated claimReviewed (na or None or empty string)\n",
    "df = df[~pd.isna(df.claimReviewed)]\n",
    "df = df[df.claimReviewed != \"\"]\n",
    "\n",
    "# Remove all Empty Dates\n",
    "df = df[~pd.isna(df.datePublished)]\n",
    "df = df[df.datePublished != \"\"]\n",
    "\n",
    "\n",
    "# Replace 20190-02-21 with 2019-02-21\n",
    "df.datePublished = pd.to_datetime(df.datePublished, utc = True, errors='coerce')\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df = df.sort_values(by=\"datePublished\")\n",
    "df.drop_duplicates(subset=\"claimReviewed\",inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df[\"id\"] = df.claimReviewed.apply(lambda x: uuid.uuid4().hex)\n",
    "df.set_index(\"id\",inplace=True)\n",
    "\n",
    "# Replace URL by domain\n",
    "get_domain = lambda x: extract(x).domain\n",
    "df[\"domain\"] = df.url.apply(lambda x: get_domain(x) if isinstance(x,str) else None)\n",
    "\n",
    "# For the rows where name is a list join with \", \"\n",
    "df[\"name\"] = df.name.apply(lambda x: \", \".join(x) if isinstance(x,list) else x)\n",
    "\n",
    "# Where there is no name take domain\n",
    "df.loc[pd.isna(df.name),\"name\"] = df.loc[pd.isna(df.name),\"domain\"]\n",
    "df.loc[df.name == \"\",\"name\"] = df.loc[df.name == \"\",\"domain\"]\n",
    "\n",
    "isna_name = pd.isna(df.name)\n",
    "isempty_name = df.name == \"\"\n",
    "isna_domain = pd.isna(df.domain)\n",
    "isempty_domain = df.domain == \"\"\n",
    "df = df[~((isna_domain | isempty_domain))]\n",
    "\n",
    "df = df[(df[\"datePublished\"] < pd.to_datetime(\"2024-03-21\", utc = True)) & (df[\"datePublished\"] > pd.to_datetime(\"2000-12-30\", utc = True))]\n",
    "\n",
    "df.author_url = df.url.apply(lambda x: geturl(x) if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {pd.isna(df.inLanguage).sum()} rows with no language\")\n",
    "# Iterate through domains and check whethe at least 95% of non NA values are the same\n",
    "# If so, replace the NA values with the most common value\n",
    "domain_counts = df.domain.value_counts()\n",
    "for domain in domain_counts.index:\n",
    "    languages = df.loc[df.domain == domain,\"inLanguage\"]\n",
    "    # Check whether less than 25 percent of the values are None\n",
    "    if pd.isna(languages).sum() / languages.shape[0] > 0.5:\n",
    "        continue\n",
    "    languages = languages[~pd.isna(languages)]    \n",
    "    # Add a check whether there are at least 50 values\n",
    "    if languages.shape[0] > 50:\n",
    "        if languages.value_counts().iloc[0] / languages.shape[0] > 0.95:\n",
    "            df.loc[(df.domain == domain) & (pd.isna(df.inLanguage)),\"inLanguage\"] = languages.value_counts().index[0]\n",
    "\n",
    "def safe_detect_langs(text):\n",
    "    try:\n",
    "        return detect_langs(text)[0].lang\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "df[\"detected_language\"] = df.claimReviewed.progress_apply(lambda x: safe_detect_langs(x))\n",
    "\n",
    "\n",
    "df.detected_language = df.detected_language.str.lower()\n",
    "df.inLanguage = df.inLanguage.str.lower()\n",
    "# Check whether the detected language is the same as the inLanguage for inLanguage != None\n",
    "df[\"detected_language_same\"] = df.apply(lambda x: x.detected_language == x.inLanguage if not pd.isna(x.inLanguage) else None,axis=1)\n",
    "\n",
    "# Print Percentage of same\n",
    "print(f\"{df.detected_language_same.sum() / sum(df.detected_language != None)} of the detected languages are the same as the inLanguage\")\n",
    "\n",
    "# Create New Variable Langauge which takes it's value from detectedLagnauge if there is no inLanguage\n",
    "df[\"language\"] = df.inLanguage\n",
    "df.loc[pd.isna(df.language),\"language\"] = df.loc[pd.isna(df.language),\"detected_language\"]\n",
    "df = df.drop(columns=[\"detected_language\",\"detected_language_same\",\"inLanguage\"])\n",
    "\n",
    "supported_languages_labse = [\"zh-tw\",\"af\",\"ar\",\"as\",\"az\",\"bn\",\"bo\", \"bs\", \"ca\", \"ceb\",\"co\", \"cs\", \"cy\", \"da\", \"de\", \"el\",\"en\", \"eo\", \"es\", \"et\", \"eu\", \"fa\", \"fi\", \"fr\", \"fy\", \"ga\", \"gd\", \"gl\", \"gu\", \"ha\", \"haw\",\"he\",\"hi\", \"hmn\",\"hr\", \"ht\", \"hu\", \"hy\", \"id\", \"ig\", \"is\", \"it\", \"ja\", \"jv\", \"ka\", \"kk\", \"km\", \"kn\", \"ko\", \"ku\", \"ky\", \"la\", \"lb\", \"lo\", \"lt\", \"lv\", \"mg\", \"mi\", \"mk\", \"ml\",\"mn\", \"mr\", \"ms\", \"mt\", \"my\", \"ne\", \"nl\", \"no\", \"ny\", \"or\", \"pa\", \"pl\", \"pt\", \"ro\", \"ru\", \"rw\", \"si\", \"sk\", \"sl\", \"sm\", \"sn\", \"so\", \"sq\", \"sr\", \"st\", \"su\", \"sv\", \"sw\", \"ta\", \"te\", \"tg\", \"th\",\"tk\", \"tl\", \"tr\", \"tt\", \"ug\", \"uk\", \"ur\",\"uz\", \"vi\",\"wo\", \"xh\", \"yi\", \"yo\", \"zh\", \"zu\"]\n",
    "print(f\"{100*df.language.isin(supported_languages_labse).mean():.2f}% of the data is in a supported language\")\n",
    "print(df[~df.language.isin(supported_languages_labse)].language.value_counts().index.to_list())\n",
    "df = df[df.language.isin(supported_languages_labse)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba1354",
   "metadata": {},
   "source": [
    "## Clean ClaimReviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode HTML Entities in the claimReviewed. Make sure that we do not delete anything that is not an entity\n",
    "def decode_html(s):\n",
    "    decoded = html.unescape(s)\n",
    "\n",
    "    # Custom replacements\n",
    "    replacements = {\n",
    "        '\\xa0': ' ',  # Non-breaking space\n",
    "        '\\u2003': ' ',  # Em space\n",
    "        '\\u2002': ' ',  # En space\n",
    "        '\\u2009': ' ',  # Thin space\n",
    "        '\\u200c': '',   # Zero width non-joiner\n",
    "        '\\u200d': ''    # Zero width joiner\n",
    "    }\n",
    "\n",
    "    for original, replacement in replacements.items():\n",
    "        decoded = decoded.replace(original, replacement)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "assert decode_html(\"‰Ω†Â•Ω &amp; ‰Ω†Â•Ω\") == \"‰Ω†Â•Ω & ‰Ω†Â•Ω\"\n",
    "assert decode_html(\"Caf&eacute; &amp; croissant\") == \"Caf√© & croissant\"\n",
    "assert decode_html(\"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø &lt;3\") == \"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø <3\"\n",
    "assert decode_html(\"–ü—Ä–∏–≤–µ—Ç, &quot;–º–∏—Ä&quot;!\") == '–ü—Ä–∏–≤–µ—Ç, \"–º–∏—Ä\"!'\n",
    "assert decode_html(\"ŸÖÿ±ÿ≠ÿ®ÿß &gt; ŸÖÿ±ÿ≠ÿ®ÿ™ŸäŸÜ\") == \"ŸÖÿ±ÿ≠ÿ®ÿß > ŸÖÿ±ÿ≠ÿ®ÿ™ŸäŸÜ\"\n",
    "assert decode_html(\"&amp;\") == \"&\"\n",
    "assert decode_html(\"&lt;\") == \"<\"\n",
    "assert decode_html(\"&gt;\") == \">\"\n",
    "assert decode_html(\"&nbsp;\") == \" \"\n",
    "assert decode_html(\"&quot;\") == '\"'\n",
    "\n",
    "df[\"claimReviewed\"] = df.claimReviewed.progress_apply(lambda x: decode_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ed56b",
   "metadata": {},
   "source": [
    "## Remove Duplicates based on minimal ClaimReviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates by mapping to the lowest resolution of the text. Make sure that it works for all languages\n",
    "\n",
    "def map_minimal(s):\n",
    "    # Remove non-alphanumeric characters except spaces using regex\n",
    "    result = re.sub(r'[^\\w]|[\\s\\n]', '', s)\n",
    "    # Remove spaces and convert the remaining characters to lowercase\n",
    "    return result.lower().replace(\" \", \"\")\n",
    "\n",
    "assert map_minimal(\"Hello? is this real\") == \"helloisthisreal\"\n",
    "assert map_minimal(\"Hello!\") == \"hello\"\n",
    "assert map_minimal(\"Hello 123\") == \"hello123\"\n",
    "assert map_minimal(\"„Åì„Çì„Å´„Å°„ÅØ„ÄÅÁßÅ„ÅÆÂêçÂâç„ÅØ„Éú„Éñ„Åß„Åô„ÄÇ\") == \"„Åì„Çì„Å´„Å°„ÅØÁßÅ„ÅÆÂêçÂâç„ÅØ„Éú„Éñ„Åß„Åô\"\n",
    "assert map_minimal(\"◊©◊ú◊ï◊ù ◊¢◊ï◊ú◊ù\") == \"◊©◊ú◊ï◊ù◊¢◊ï◊ú◊ù\"\n",
    "assert map_minimal(\"‰Ω†Â•ΩÔºåÊàëÂè´Â∞èÊòé„ÄÇ\") == \"‰Ω†Â•ΩÊàëÂè´Â∞èÊòé\"\n",
    "assert map_minimal(\"–ü—Ä–∏–≤–µ—Ç, –º–µ–Ω—è –∑–æ–≤—É—Ç –ê–Ω–¥—Ä–µ–π.\") == \"–ø—Ä–∏–≤–µ—Ç–º–µ–Ω—è–∑–æ–≤—É—Ç–∞–Ω–¥—Ä–µ–π\"\n",
    "assert map_minimal(\"ÏïàÎÖïÌïòÏÑ∏Ïöî. Ï†ú Ïù¥Î¶ÑÏùÄ ÌôçÍ∏∏ÎèôÏûÖÎãàÎã§.\") == \"ÏïàÎÖïÌïòÏÑ∏ÏöîÏ†úÏù¥Î¶ÑÏùÄÌôçÍ∏∏ÎèôÏûÖÎãàÎã§\"\n",
    "assert map_minimal(\"ŸÖÿ±ÿ≠ÿ®ÿßÿå ÿßÿ≥ŸÖŸä ÿ£ÿ≠ŸÖÿØ.\") == \"ŸÖÿ±ÿ≠ÿ®ÿßÿßÿ≥ŸÖŸäÿ£ÿ≠ŸÖÿØ\"\n",
    "assert map_minimal(\"Hello#World\") == \"helloworld\"\n",
    "assert map_minimal(\"Hello Wor*ld\") == \"helloworld\"\n",
    "assert map_minimal(\"Hello\\nWorld\") == \"helloworld\"\n",
    "assert map_minimal(\"\") == \"\"\n",
    "assert map_minimal(\" \") == \"\"\n",
    "assert map_minimal(\"üëç This is a test üòÄ of emojis! üëã\") == \"thisisatestofemojis\"\n",
    "assert map_minimal(\"[Square brackets], {curly braces}, and <angle brackets> too!\") == \"squarebracketscurlybracesandanglebracketstoo\"\n",
    "assert map_minimal(\"„Äå‰Ω†Â•ΩÂóéÔºü„Äç\") == \"‰Ω†Â•ΩÂóé\"\n",
    "assert map_minimal(\"„Äê„Äë„Äå„ÄçHello\") == \"hello\"\n",
    "\n",
    "# Remove all claims with less than 5 characters\n",
    "df = df[df.claimReviewed.str.len() > 5]\n",
    "df[\"claim_minimal\"] = df[\"claimReviewed\"].progress_apply(lambda x: map_minimal(x))\n",
    "np.random.seed(1)\n",
    "df = df.sample(frac=1).drop_duplicates(subset=\"claim_minimal\", keep=\"first\").sort_index().reset_index(drop = True)\n",
    "df[\"verdict\"] = df.verdict.map(lambda x: verdict_mapping[x] if x in verdict_mapping else \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Data/Cleaned_FactCheckData_nopreprocess_local.csv.gz\", compression=\"gzip\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
