{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bb7dd-a118-4f32-8afe-6be3d1346a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpcore\n",
    "setattr(httpcore, 'SyncHTTPTransport', any)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "from urllib.parse import urlparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import glob\n",
    "import requests\n",
    "import uuid\n",
    "from langdetect import detect, DetectorFactory\n",
    "import re\n",
    "from googletrans import Translator\n",
    "from tldextract import extract\n",
    "from langdetect import detect_langs\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import html\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(https?://(?:www\\.|)twitter\\.com/[\\w\\d_/]+|https?://t\\.co/[\\w\\d]+)'\n",
    "# Define a function to find matches and create a dictionary\n",
    "def find_twitter_links(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return {}\n",
    "    matches = re.findall(pattern, text)\n",
    "    # Create a dictionary where keys are the match order (1-indexed) and values are the matches\n",
    "    return {i+1: match for i, match in enumerate(matches)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FACTCHECKS\n",
    "link = \"https://storage.googleapis.com/datacommons-feeds/factcheck/latest/data.json\"\n",
    "# Download JSON with requests\n",
    "r = requests.get(link)\n",
    "# Load JSON\n",
    "factchecks = r.json()\n",
    "\n",
    "columns = [\"datePublished\",\"claimReviewed\",\"author\"]\n",
    "temp_df = pd.concat(map(pd.DataFrame,[x[\"item\"] for x in factchecks[\"dataFeedElement\"]]))\n",
    "temp_df = temp_df[columns]\n",
    "temp_df[\"name\"] = temp_df[\"author\"].apply(lambda x: x.get(\"name\",None) if isinstance(x,dict) else None)\n",
    "temp_df[\"url\"] = temp_df[\"author\"].apply(lambda x: x.get(\"url\",None) if isinstance(x,dict) else None)\n",
    "temp_df[\"twitter_urls\"] = temp_df[\"claimReviewed\"].apply(lambda x: find_twitter_links(x))\n",
    "temp_df[\"@type\"] = temp_df[\"author\"].apply(lambda x: x.get(\"@type\",None) if isinstance(x,dict) else None)\n",
    "temp_df.drop(columns=[\"author\"],inplace=True)\n",
    "temp_df = temp_df[~pd.isna(temp_df.claimReviewed)]\n",
    "temp_df[\"claimReviewed\"] = temp_df[\"claimReviewed\"].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "temp_df.reset_index(drop=True,inplace=True)\n",
    "google_dump = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FACTCHECKS\n",
    "link = \"https://storage.googleapis.com/datacommons-feeds/factcheck/latest/data.json\"\n",
    "# Download JSON with requests\n",
    "r = requests.get(link)\n",
    "# Load JSON\n",
    "factchecks = r.json()\n",
    "columns = [\"datePublished\",\"claimReviewed\",\"name\", \"verdict\",\"author_url\", \"url\",\"twitter_urls\"]\n",
    "\n",
    "# Prepare empty list to store dataframes\n",
    "dfs = []\n",
    "# Iterate over each DataFeedItem in dataFeedElement\n",
    "for data_item in tqdm(factchecks[\"dataFeedElement\"]):\n",
    "    # Create temporary DataFrame for each item\n",
    "    temp_df = pd.DataFrame(data_item[\"item\"])\n",
    "    \n",
    "    if 'author' in temp_df.columns and \"claimReviewed\" in temp_df.columns:\n",
    "        temp_df[\"name\"] = temp_df[\"author\"].apply(lambda x: x.get(\"name\",None) if isinstance(x,dict) else None)\n",
    "        temp_df[\"author_url\"] = temp_df[\"author\"].apply(lambda x: x.get(\"url\",None) if isinstance(x,dict) else None)\n",
    "        temp_df[\"@type\"] = temp_df[\"author\"].apply(lambda x: x.get(\"@type\",None) if isinstance(x,dict) else None)\n",
    "        temp_df[\"twitter_urls\"] = temp_df[\"claimReviewed\"].apply(lambda x: find_twitter_links(x))\n",
    "        if \"reviewRating\" in temp_df.columns:\n",
    "            temp_df[\"verdict\"] = temp_df[\"reviewRating\"].apply(lambda x: x[\"alternateName\"] if isinstance(x,dict) and \"alternateName\" in x else None)\n",
    "        else:\n",
    "            temp_df[\"verdict\"] = None\n",
    "        temp_df.drop(columns=[\"author\"],inplace=True)\n",
    "\n",
    "    temp_df[\"url\"] = data_item[\"url\"]  # Add URL from DataFeedItem to each row in the DataFrame\n",
    "    dfs.append(temp_df)  # Append temporary DataFrame to list\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Filter rows where claimReviewed is not NaN\n",
    "df = df[~pd.isna(df.claimReviewed)]\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "google_dump = df\n",
    "google_dump = google_dump[columns]\n",
    "google_dump[\"inLanguage\"] = None\n",
    "print(google_dump.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic:\n",
    "# If text is shorter than < 50 take it as claim else claim\n",
    "def decide(x):\n",
    "    \"\"\"\n",
    "    Decides which part of the Fact-Check is a claim. \n",
    "    Preference Order: Claim > Body > Headline\n",
    "    \"\"\"\n",
    "    \n",
    "    if x.claimReviewed:\n",
    "        return(x.claimReviewed)\n",
    "    else:\n",
    "        if x.headline and isinstance(x.headline,str):\n",
    "            if len(x.headline) <= average_length_claim_reviewd + 1.96*var_claim_reviewd**0.5:\n",
    "                return(x.headline)\n",
    "        if x.text and isinstance(x.text,str):\n",
    "            if len(x.text) <= average_length_claim_reviewd + 1.96*var_claim_reviewd**0.5:\n",
    "                return(x.text)\n",
    "        else:\n",
    "            return(float(\"nan\"))\n",
    "    return(float(\"nan\"))\n",
    "\n",
    "# Calulate the Average Length & Variance Based on the Google Dump\n",
    "average_length_claim_reviewd = google_dump.claimReviewed.apply(lambda x: len(x)).mean()\n",
    "var_claim_reviewd = google_dump.claimReviewed.apply(lambda x: len(x)).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d520556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_and_load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        file_contents = f.read()\n",
    "\n",
    "    # Assuming JSON objects are separated by line breaks for this example\n",
    "    potential_json_objects = file_contents.split('\\n')\n",
    "\n",
    "    valid_json_objects = []\n",
    "    for obj_str in potential_json_objects:\n",
    "        try:\n",
    "            json_obj = json.loads(obj_str)\n",
    "            valid_json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Found an invalid JSON object, skipping...\")\n",
    "\n",
    "    return valid_json_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "count = 0\n",
    "# Columns to extract\n",
    "columns = [\"datePublished\",\"claimReviewed\",\"name\",\"author_url\", \"url\",\"twitter_urls\",\"inLanguage\",\"verdict\", \"claim_date\",\"text\"]\n",
    "\n",
    "# Loop through all Dataframe\n",
    "for f in tqdm(os.listdir(\"../Data/Factcheck/\")):\n",
    "    if \".json\" in f:\n",
    "        if \"2024\" in f:\n",
    "            temp_json = fix_and_load_json(\"../Data/Factcheck/{}\".format(f))\n",
    "            temp_df = [x[\"_source\"] for x in temp_json if \"_source\" in x]\n",
    "            temp_df = pd.DataFrame(temp_df)\n",
    "\n",
    "            temp_df[\"twitter_urls\"] = temp_df[\"claim_review_body\"].apply(lambda x: find_twitter_links(x))\n",
    "            temp_df[\"claimReviewed\"] = temp_df[\"claim_review_headline\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "            temp_df[\"text\"] = temp_df[\"claim_review_body\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "            temp_df[\"headline\"] = temp_df[\"claim_review_headline\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "\n",
    "            temp_df[\"verdict\"] = temp_df[\"claim_review_result\"]\n",
    "            temp_df[\"author_url\"] = temp_df[\"author_link\"]\n",
    "            temp_df[\"name\"] = temp_df[\"author\"]\n",
    "            temp_df[\"url\"] = temp_df[\"claim_review_url\"]\n",
    "            temp_df[\"datePublished\"] = pd.to_datetime(temp_df[\"created_at\"], errors='coerce')\n",
    "            temp_df[\"inLanguage\"] = temp_df[\"language\"]\n",
    "            try:\n",
    "                raw_claim_review = temp_df[\"raw_claim_review\"].apply(lambda x: json.loads(x) if isinstance(x,str) else x)\n",
    "                temp_df[\"claim_date\"] = raw_claim_review.apply(lambda x: x.get(\"claimDate\",None) if isinstance(x,dict) else None)\n",
    "            except Exception as e:\n",
    "                print(f\"\\033[91mError in New: {e}\\033[0m\")\n",
    "                temp_df[\"claim_date\"] = None\n",
    "\n",
    "            temp_df[\"author\"] = temp_df[\"author\"].apply(lambda x: \", \".join(x) if isinstance(x,list) else str(x))\n",
    "            temp_df[\"author_link\"] = temp_df[\"author_link\"].apply(lambda x: \"\".join(x) if isinstance(x,str) else str(x))\n",
    "        else:\n",
    "            temp_json = pd.read_json(\"../Data/Factcheck/{}\".format(f))\n",
    "            temp_df = pd.DataFrame(temp_json)\n",
    "\n",
    "            temp_df[\"twitter_urls\"] = temp_df[\"raw\"].apply(lambda x: find_twitter_links(x[\"claim_review_body\"]))\n",
    "            temp_df[\"claimReviewed\"] = temp_df[\"claimReviewed\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "            temp_df[\"text\"] = temp_df[\"text\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "            temp_df[\"headline\"] = temp_df[\"headline\"].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x,str) else x)\n",
    "\n",
    "            temp_df[\"verdict\"] = temp_df[\"reviewRating\"].apply(lambda x: x.get(\"alternateName\",None) if isinstance(x,dict) else None)\n",
    "\n",
    "            if \"author\" in temp_df.columns:\n",
    "                temp_df[\"author_url\"] = temp_df.author.apply(lambda x: x.get(\"url\",None) if isinstance(x,dict) else None)\n",
    "                temp_df[\"name\"] = temp_df[\"author\"].apply(lambda x: x.get(\"name\",None) if isinstance(x,dict) else None)\n",
    "            else:\n",
    "                temp_df[\"author_url\"] = None\n",
    "                temp_df[\"name\"] = None\n",
    "\n",
    "            temp_df[\"url\"] = temp_df[\"url\"]\n",
    "            temp_df[\"datePublished\"] = pd.to_datetime(temp_df[\"datePublished\"], errors='coerce')\n",
    "            temp_df[\"inLanguage\"] = temp_df[\"inLanguage\"]\n",
    "            temp_df[\"raw_claim_review\"] = temp_df.apply(lambda x: json.loads(x.get(\"raw\",{}).get(\"raw_claim_review\",\"{}\")),axis=1)\n",
    "            temp_df[\"claim_date\"] = temp_df[\"raw_claim_review\"].apply(lambda x: x.get(\"claimDate\", None) if x else None)\n",
    "        \n",
    "        count += temp_df.shape[0]\n",
    "\n",
    "        # Extract which column is the claimReviewed\n",
    "        for i,r in temp_df.iterrows():\n",
    "            temp_df.loc[i,\"claimReviewed\"] = decide(r)\n",
    "        \n",
    "        temp_df = temp_df[columns]    \n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        all_dfs.append(temp_df)\n",
    "        \n",
    "scraped_factchecks = pd.concat(all_dfs)\n",
    "scraped_factchecks = scraped_factchecks.reset_index(drop = True)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2ff01",
   "metadata": {},
   "source": [
    "### Analysis of Duration to Publish Fact-check - We should do this when we have the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_factchecks[\"claim_date\"] = pd.to_datetime(scraped_factchecks[\"claim_date\"], errors='coerce', utc=True)\n",
    "scraped_factchecks[\"datePublished\"] = pd.to_datetime(scraped_factchecks[\"datePublished\"], errors='coerce', utc=True)\n",
    "scraped_factchecks[\"timedifference\"] = abs(scraped_factchecks[\"claim_date\"] - scraped_factchecks[\"datePublished\"]).apply(lambda x: x.total_seconds() if not pd.isna(x) else x)\n",
    "scraped_factchecks.loc[scraped_factchecks[\"timedifference\"] < 3600,\"timedifference\"] = None\n",
    "scraped_factchecks[\"timedifference_hours\"] = scraped_factchecks[\"timedifference\"] // 3600\n",
    "scraped_factchecks[\"timedifference_days\"] = scraped_factchecks[\"timedifference\"] // (3600 * 24)\n",
    "scraped_factchecks[\"author_url\"] = scraped_factchecks[\"author_url\"].apply(lambda x: \", \".join(x) if isinstance(x,list) else str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0181f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Ratings:\n",
    "# - Only look at Ratings that have been given at least 100 times\n",
    "# - remove all punctuation, case, and spaces\n",
    "scraped_factchecks[\"verdict\"] = scraped_factchecks[\"verdict\"].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "scraped_factchecks[\"verdict\"] = scraped_factchecks[\"verdict\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x) if isinstance(x,str) else x)\n",
    "all_verdicts = scraped_factchecks[\"verdict\"].value_counts()\n",
    "all_verdicts = all_verdicts[all_verdicts > 50]\n",
    "verdict_mapping = {\"false\": \"False\", \"falso\": \"False\", \"misleading\": \"Mostly False\", \n",
    "    \"مضلل\": \"Mostly False\", \"mostly false\": \"Mostly False\", \"half true\": \"Mostly True\", \n",
    "    \"yanlış\": \"False\", \"زائف\": \"False\", \"true\": \"True\", \"خطأ\": \"False\", \n",
    "    \"fake\": \"False\", \"錯誤\": \"False\", \"faux\": \"False\", \"misleading content\": \"Mostly False\",\n",
    "    \"engañoso\": \"Mostly False\", \"fałsz\": \"False\", \"verdadeiro\": \"True\",\n",
    "    \"false context\": \"False\", \"errado\": \"False\", \"yanlis\": \"False\", \n",
    "    \"salah misleading content\": \"Mostly False\", \"mentira\": \"False\", \n",
    "    \"mixture\": \"NA\", \"fabricated content\": \"False\", \"pants on fire\": \"False\", \n",
    "    \"partly false\": \"Mostly False\", \"incorrect\": \"False\", \"other\": \"NA\",\n",
    "    \"manipulated content\": \"False\", \"keliru\": \"False\", \"salah false context\": \"False\",\n",
    "    \"2 false context\": \"False\", \"falsch\": \"False\", \"missing context\": \"Mostly False\", \n",
    "    \"notizia falsa\": \"False\", \"2 misleading content\": \"Mostly False\", \n",
    "    \"predominantemente falso\": \"False\", \"prawda\": \"True\", \"salah\": \"False\", \n",
    "    \"enganoso\": \"Mostly False\", \"παραπληροφόρηση\": \"Mostly False\", \"fals\": \"False\", \n",
    "    \"verdadeiro mas\": \"Mostly True\", \"fact crescendo rating false\": \"False\",\n",
    "    \"mostly true\": \"Mostly True\", \"yanliş\": \"False\", \"verdadero\": \"True\", \n",
    "    \"salah fabricated content\": \"False\", \"satire\": \"NA\", \"doğru\": \"True\", \n",
    "    \"unproven\": \"NA\", \"部分錯誤\": \"Mostly False\", \"sesat\": \"False\", \n",
    "    \"impostor content\": \"False\", \"fuori contesto\": \"Mostly False\", \"nepravda\": \"False\", \n",
    "    \"不實\": \"False\", \"2 manipulated content\": \"False\", \"salah manipulated content\": \"False\",\n",
    "    \"2 fabricated content\": \"False\", \"enganyós\": \"Mostly False\", \"ψευδές\": \"False\", \n",
    "    \"salah impostor content\": \"False\", \"impreciso\": \"Mostly False\", \"inconclusive\": \"NA\",\n",
    "    \"pimenta na língua\": \"NA\", \"صحيح\": \"True\", \"falsk\": \"False\", \"netačno\": \"False\",\n",
    "    \"kısmen yanlış\": \"Mostly False\", \"trompeur\": \"Mostly False\", \"manipulacja\": \"False\",\n",
    "    \"correct\": \"True\", \"nì\": \"NA\", \"إثارة\": \"NA\", \"miscaptioned\": \"False\", \n",
    "    \"correct attribution\": \"True\", \"clarification\": \"NA\", \"cuestionable\": \"NA\", \n",
    "    \"inaccurate\": \"Mostly False\", \"altered image\": \"False\", \"cest faux\": \"False\",\n",
    "    \"misplaced context\": \"Mostly False\", \"vero\": \"True\", \"helt feil\": \"False\", \n",
    "    \"sin registro\": \"NA\", \"ساخر\": \"NA\", \"altered\": \"False\", \"pinocchio andante\": \"False\",\n",
    "    \"tak benar\": \"False\", \"事實釐清\": \"NA\", \"karma\": \"NA\", \"enganador\": \"Mostly False\", \n",
    "    \"meiaverdade\": \"NA\", \"ceri quasi\": \"NA\", \"irreführend\": \"Mostly False\",\n",
    "    \"no evidence\": \"NA\", \"distorcido\": \"Mostly False\", \"مركب\": \"False\", \n",
    "    \"checked\": \"NA\", \"four pinocchios\": \"False\", \"false  content that has no basis in fact\": \"False\", \n",
    "    \"needs context\": \"NA\", \"fabricated\": \"False\", \"false connection\": \"False\",\n",
    "    \"عنوان مضلل\": \"False\", \"raczej fałsz\": \"Mostly False\", \"ikke dokumenteret\": \"NA\", \n",
    "    \"tvrdnja je netočna\": \"False\", \"rrenë\": \"False\", \"گمراہ کن دعوی\": \"NA\", \n",
    "    \"exagerado\": \"NA\", \"фейк\": \"False\", \"neistina\": \"False\", \"falta contexto\": \"Mostly False\", \n",
    "    \"unsupported\": \"NA\", \"правда\": \"True\", \"فرضی دعوی\": \"NA\", \"манипуляция\": \"False\",\n",
    "    \"كذب\": \"False\", \"ψευδής ισχυρισμός\": \"False\", \"انتقائي\": \"Mostly False\", \n",
    "    \"suspicious\": \"NA\", \"manipulated\": \"False\", \"مركبة\": \"False\", \"5 clarification\": \"NA\", \n",
    "    \"scam\": \"False\", \"mostly correct\": \"Mostly True\", \"półprawda\": \"Mostly True\", \n",
    "    \"chequeo múltiple\": \"NA\", \"sem contexto\": \"Mostly False\", \"تضليل\": \"Mostly False\",\n",
    "    \"भरमक\": \"NA\", \"distorts the facts\": \"Mostly False\", \"salah satire\": \"False\", \"3\": \"NA\", \n",
    "    \"three pinocchios\": \"Mostly False\", \"verdadero pero\": \"Mostly True\", \n",
    "    \"commotion\": \"NA\", \"parcialmente falso\": \"Mostly False\", \"explainer\": \"NA\",\n",
    "    \"verdadeiro em partes\": \"Mostly True\", \"nieweryfikowalne\": \"NA\", \n",
    "    \"false  the primary claims of the content are factually inaccurate\": \"False\",\n",
    "    \"mix\": \"NA\", \"hard to categorise\": \"NA\", \"notizia vera\": \"True\", \n",
    "    \"real\": \"True\", \"false and misleading\": \"False\", \"selective\": \"Mostly False\", \n",
    "    \"مشكوك فيه\": \"NA\", \"ложь\": \"False\", \"misleading and false\": \"False\", \n",
    "    \"زائف جزئيا\": \"Mostly False\", \"ψευδοεπιστήμη\": \"NA\", \"manipulated media\": \"False\",\n",
    "    \"partially true\": \"Mostly True\", \"misvisende\": \"Mostly False\", \"rrenë e kryptë\": \"False\", \n",
    "    \"verificamos\": \"NA\", \"verdad a medias\": \"Mostly True\", \n",
    "    \"λείπει θεματικό περιεχόμενο\": \"NA\", \"hetimet vazhdojnë\": \"NA\",\n",
    "    \"two pinocchios\": \"Mostly False\", \"نادرست\": \"False\", \"descontextualizado\": \"Mostly False\", \n",
    "    \"sem registro\": \"NA\", \"misattributed\": \"False\", \"labeled satire\": \"NA\", \n",
    "    \"לא נכון\": \"False\", \"2 false connection\": \"False\", \"hoax\": \"False\", \n",
    "    \"falsk men\": \"Mostly False\", \"false headline\": \"False\", \"partiellement faux\": \"Mostly False\", \n",
    "    \"panzana pazzesca\": \"False\", \"5050\": \"NA\", \"outdated\": \"NA\", \n",
    "    \"montaje\": \"False\", \"غير صحيح\": \"False\", \"immagine modificata\": \"False\", \n",
    "    \"خرافة\": \"False\", \"false claim\": \"False\", \"flipflop\": \"NA\", \"очень спорно\": \"NA\", \n",
    "    \"not the whole story\": \"Mostly False\", \"certo\": \"True\", \"1 true\": \"True\",\n",
    "    \"falsa\": \"False\", \"mostly true  mostly accurate but there is a minor error or problem\": \"Mostly True\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80226f",
   "metadata": {},
   "source": [
    "## Extract Twitter Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09494b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_scraped_dict = scraped_factchecks.set_index(\"url\")[\"twitter_urls\"]\n",
    "url_scraped_dict = {k:v for k,v in url_scraped_dict.items() if len(v) > 0}\n",
    "url_scraped_dict\n",
    "all_urls = [list(value.values()) for key, value in url_scraped_dict.items()]\n",
    "# Flatten the list\n",
    "all_urls = [item for sublist in all_urls for item in sublist]\n",
    "# Remove duplicates\n",
    "all_urls = list(set(all_urls))\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))\n",
    "def get_final_url(url):\n",
    "    # Check if url is a t.co link\n",
    "    if \"t.co\" in url:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            return url, response.url\n",
    "        except (requests.exceptions.RequestException, UnicodeDecodeError):\n",
    "            return url, url\n",
    "    else:\n",
    "        return url, url\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    url_mapping = dict(tqdm(executor.map(get_final_url, all_urls), total=len(all_urls)))\n",
    "\n",
    "\n",
    "url_scraped_dict_resolved = {}\n",
    "for key in tqdm(url_scraped_dict):\n",
    "    for number, link in url_scraped_dict[key].items():\n",
    "        if key not in url_scraped_dict_resolved:\n",
    "            url_scraped_dict_resolved[key] = {}\n",
    "        if \"t.co\" in link:\n",
    "            if link in url_mapping and url_mapping[link] != link and \"t.co\" not in url_mapping[link]:\n",
    "                to_add = {\"Resolved\":url_mapping[link], \"Original\":link}\n",
    "            else:\n",
    "                to_add = {\"Resolved\":link, \"Original\":link}\n",
    "            url_scraped_dict_resolved[key][number] = to_add\n",
    "        else:\n",
    "            url_scraped_dict_resolved[key][number] = {\"Resolved\":link, \"Original\":link}\n",
    "\n",
    "# First, convert the nested dictionary into a flat list of dictionaries\n",
    "rows_list = []\n",
    "for article_url, urls in url_scraped_dict_resolved.items():\n",
    "    for idx, url_dict in urls.items():\n",
    "        row_dict = {\n",
    "            'Article_Url': article_url,\n",
    "            'Index': idx,\n",
    "            'Original': url_dict['Original'],\n",
    "            'Resolved': url_dict['Resolved']\n",
    "        }\n",
    "        rows_list.append(row_dict)\n",
    "\n",
    "# Now, convert the list of dictionaries into a DataFrame\n",
    "url_df = pd.DataFrame(rows_list)\n",
    "url_df[\"Twitter_Link\"] = url_df[\"Resolved\"].apply(lambda x: True if \"twitter.com\" in x else False)\n",
    "url_df[\"Twitter_Id\"] = url_df[\"Resolved\"].apply(lambda x: x.split(\"/\")[-1] if \"twitter.com\" in x and \"status\" in x else None)\n",
    "\n",
    "# For each link set variable is_first to True if it is the first link with an associated Twitter ID in the article\n",
    "url_df = url_df[(url_df.Twitter_Id.notnull()) & (url_df.Twitter_Id.apply(lambda x: len(x) == 19 if isinstance(x,str) else False))]\n",
    "first_urls = url_df.groupby(\"Article_Url\").apply(lambda x: x[\"Index\"] == x[\"Index\"].min())\n",
    "first_urls = first_urls.reset_index(level=1, drop=True).to_dict()\n",
    "first_urls = {k for k,v in first_urls.items() if v}\n",
    "url_df = url_df[url_df.Article_Url.isin(first_urls)]\n",
    "# Export to CSV\n",
    "url_df.to_csv(\"../Data/URLs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86063281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geturl(x):\n",
    "    return extract(x).domain + \".\" + extract(x).suffix\n",
    "\n",
    "# Add inLanguage to Google Dump\n",
    "google_dump[\"inLanguage\"] = None\n",
    "\n",
    "# Merge Google Dump with other Factchecks\n",
    "df = pd.concat([scraped_factchecks,google_dump])\n",
    "\n",
    "# Remove all Rows with no associated claimReviewed (na or None or empty string)\n",
    "df = df[~pd.isna(df.claimReviewed)]\n",
    "df = df[df.claimReviewed != \"\"]\n",
    "\n",
    "# Remove all Empty Dates\n",
    "df = df[~pd.isna(df.datePublished)]\n",
    "df = df[df.datePublished != \"\"]\n",
    "\n",
    "\n",
    "# Replace 20190-02-21 with 2019-02-21\n",
    "df.datePublished = pd.to_datetime(df.datePublished, utc = True, errors='coerce')\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df = df.sort_values(by=\"datePublished\")\n",
    "df.drop_duplicates(subset=\"claimReviewed\",inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df[\"id\"] = df.claimReviewed.apply(lambda x: uuid.uuid4().hex)\n",
    "df.set_index(\"id\",inplace=True)\n",
    "\n",
    "# Replace URL by domain\n",
    "get_domain = lambda x: extract(x).domain\n",
    "df[\"domain\"] = df.url.apply(lambda x: get_domain(x) if isinstance(x,str) else None)\n",
    "\n",
    "# For the rows where name is a list join with \", \"\n",
    "df[\"name\"] = df.name.apply(lambda x: \", \".join(x) if isinstance(x,list) else x)\n",
    "\n",
    "# Where there is no name take domain\n",
    "df.loc[pd.isna(df.name),\"name\"] = df.loc[pd.isna(df.name),\"domain\"]\n",
    "df.loc[df.name == \"\",\"name\"] = df.loc[df.name == \"\",\"domain\"]\n",
    "\n",
    "isna_name = pd.isna(df.name)\n",
    "isempty_name = df.name == \"\"\n",
    "isna_domain = pd.isna(df.domain)\n",
    "isempty_domain = df.domain == \"\"\n",
    "df = df[~((isna_domain | isempty_domain))]\n",
    "\n",
    "df = df[(df[\"datePublished\"] < pd.to_datetime(\"2024-03-21\", utc = True)) & (df[\"datePublished\"] > pd.to_datetime(\"2000-12-30\", utc = True))]\n",
    "\n",
    "df.author_url = df.url.apply(lambda x: geturl(x) if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {pd.isna(df.inLanguage).sum()} rows with no language\")\n",
    "# Iterate through domains and check whethe at least 95% of non NA values are the same\n",
    "# If so, replace the NA values with the most common value\n",
    "domain_counts = df.domain.value_counts()\n",
    "for domain in domain_counts.index:\n",
    "    languages = df.loc[df.domain == domain,\"inLanguage\"]\n",
    "    # Check whether less than 25 percent of the values are None\n",
    "    if pd.isna(languages).sum() / languages.shape[0] > 0.5:\n",
    "        continue\n",
    "    languages = languages[~pd.isna(languages)]    \n",
    "    # Add a check whether there are at least 50 values\n",
    "    if languages.shape[0] > 50:\n",
    "        if languages.value_counts().iloc[0] / languages.shape[0] > 0.95:\n",
    "            df.loc[(df.domain == domain) & (pd.isna(df.inLanguage)),\"inLanguage\"] = languages.value_counts().index[0]\n",
    "\n",
    "def safe_detect_langs(text):\n",
    "    try:\n",
    "        return detect_langs(text)[0].lang\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "df[\"detected_language\"] = df.claimReviewed.progress_apply(lambda x: safe_detect_langs(x))\n",
    "\n",
    "\n",
    "df.detected_language = df.detected_language.str.lower()\n",
    "df.inLanguage = df.inLanguage.str.lower()\n",
    "# Check whether the detected language is the same as the inLanguage for inLanguage != None\n",
    "df[\"detected_language_same\"] = df.apply(lambda x: x.detected_language == x.inLanguage if not pd.isna(x.inLanguage) else None,axis=1)\n",
    "\n",
    "# Print Percentage of same\n",
    "print(f\"{df.detected_language_same.sum() / sum(df.detected_language != None)} of the detected languages are the same as the inLanguage\")\n",
    "\n",
    "# Create New Variable Langauge which takes it's value from detectedLagnauge if there is no inLanguage\n",
    "df[\"language\"] = df.inLanguage\n",
    "df.loc[pd.isna(df.language),\"language\"] = df.loc[pd.isna(df.language),\"detected_language\"]\n",
    "df = df.drop(columns=[\"detected_language\",\"detected_language_same\",\"inLanguage\"])\n",
    "\n",
    "supported_languages_labse = [\"zh-tw\",\"af\",\"ar\",\"as\",\"az\",\"bn\",\"bo\", \"bs\", \"ca\", \"ceb\",\"co\", \"cs\", \"cy\", \"da\", \"de\", \"el\",\"en\", \"eo\", \"es\", \"et\", \"eu\", \"fa\", \"fi\", \"fr\", \"fy\", \"ga\", \"gd\", \"gl\", \"gu\", \"ha\", \"haw\",\"he\",\"hi\", \"hmn\",\"hr\", \"ht\", \"hu\", \"hy\", \"id\", \"ig\", \"is\", \"it\", \"ja\", \"jv\", \"ka\", \"kk\", \"km\", \"kn\", \"ko\", \"ku\", \"ky\", \"la\", \"lb\", \"lo\", \"lt\", \"lv\", \"mg\", \"mi\", \"mk\", \"ml\",\"mn\", \"mr\", \"ms\", \"mt\", \"my\", \"ne\", \"nl\", \"no\", \"ny\", \"or\", \"pa\", \"pl\", \"pt\", \"ro\", \"ru\", \"rw\", \"si\", \"sk\", \"sl\", \"sm\", \"sn\", \"so\", \"sq\", \"sr\", \"st\", \"su\", \"sv\", \"sw\", \"ta\", \"te\", \"tg\", \"th\",\"tk\", \"tl\", \"tr\", \"tt\", \"ug\", \"uk\", \"ur\",\"uz\", \"vi\",\"wo\", \"xh\", \"yi\", \"yo\", \"zh\", \"zu\"]\n",
    "print(f\"{100*df.language.isin(supported_languages_labse).mean():.2f}% of the data is in a supported language\")\n",
    "print(df[~df.language.isin(supported_languages_labse)].language.value_counts().index.to_list())\n",
    "df = df[df.language.isin(supported_languages_labse)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba1354",
   "metadata": {},
   "source": [
    "## Clean ClaimReviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode HTML Entities in the claimReviewed. Make sure that we do not delete anything that is not an entity\n",
    "def decode_html(s):\n",
    "    decoded = html.unescape(s)\n",
    "\n",
    "    # Custom replacements\n",
    "    replacements = {\n",
    "        '\\xa0': ' ',  # Non-breaking space\n",
    "        '\\u2003': ' ',  # Em space\n",
    "        '\\u2002': ' ',  # En space\n",
    "        '\\u2009': ' ',  # Thin space\n",
    "        '\\u200c': '',   # Zero width non-joiner\n",
    "        '\\u200d': ''    # Zero width joiner\n",
    "    }\n",
    "\n",
    "    for original, replacement in replacements.items():\n",
    "        decoded = decoded.replace(original, replacement)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "assert decode_html(\"你好 &amp; 你好\") == \"你好 & 你好\"\n",
    "assert decode_html(\"Caf&eacute; &amp; croissant\") == \"Café & croissant\"\n",
    "assert decode_html(\"আমি ভালোবাসি &lt;3\") == \"আমি ভালোবাসি <3\"\n",
    "assert decode_html(\"Привет, &quot;мир&quot;!\") == 'Привет, \"мир\"!'\n",
    "assert decode_html(\"مرحبا &gt; مرحبتين\") == \"مرحبا > مرحبتين\"\n",
    "assert decode_html(\"&amp;\") == \"&\"\n",
    "assert decode_html(\"&lt;\") == \"<\"\n",
    "assert decode_html(\"&gt;\") == \">\"\n",
    "assert decode_html(\"&nbsp;\") == \" \"\n",
    "assert decode_html(\"&quot;\") == '\"'\n",
    "\n",
    "df[\"claimReviewed\"] = df.claimReviewed.progress_apply(lambda x: decode_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ed56b",
   "metadata": {},
   "source": [
    "## Remove Duplicates based on minimal ClaimReviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates by mapping to the lowest resolution of the text. Make sure that it works for all languages\n",
    "\n",
    "def map_minimal(s):\n",
    "    # Remove non-alphanumeric characters except spaces using regex\n",
    "    result = re.sub(r'[^\\w]|[\\s\\n]', '', s)\n",
    "    # Remove spaces and convert the remaining characters to lowercase\n",
    "    return result.lower().replace(\" \", \"\")\n",
    "\n",
    "assert map_minimal(\"Hello? is this real\") == \"helloisthisreal\"\n",
    "assert map_minimal(\"Hello!\") == \"hello\"\n",
    "assert map_minimal(\"Hello 123\") == \"hello123\"\n",
    "assert map_minimal(\"こんにちは、私の名前はボブです。\") == \"こんにちは私の名前はボブです\"\n",
    "assert map_minimal(\"שלום עולם\") == \"שלוםעולם\"\n",
    "assert map_minimal(\"你好，我叫小明。\") == \"你好我叫小明\"\n",
    "assert map_minimal(\"Привет, меня зовут Андрей.\") == \"приветменязовутандрей\"\n",
    "assert map_minimal(\"안녕하세요. 제 이름은 홍길동입니다.\") == \"안녕하세요제이름은홍길동입니다\"\n",
    "assert map_minimal(\"مرحبا، اسمي أحمد.\") == \"مرحبااسميأحمد\"\n",
    "assert map_minimal(\"Hello#World\") == \"helloworld\"\n",
    "assert map_minimal(\"Hello Wor*ld\") == \"helloworld\"\n",
    "assert map_minimal(\"Hello\\nWorld\") == \"helloworld\"\n",
    "assert map_minimal(\"\") == \"\"\n",
    "assert map_minimal(\" \") == \"\"\n",
    "assert map_minimal(\"👍 This is a test 😀 of emojis! 👋\") == \"thisisatestofemojis\"\n",
    "assert map_minimal(\"[Square brackets], {curly braces}, and <angle brackets> too!\") == \"squarebracketscurlybracesandanglebracketstoo\"\n",
    "assert map_minimal(\"「你好嗎？」\") == \"你好嗎\"\n",
    "assert map_minimal(\"【】「」Hello\") == \"hello\"\n",
    "\n",
    "# Remove all claims with less than 5 characters\n",
    "df = df[df.claimReviewed.str.len() > 5]\n",
    "df[\"claim_minimal\"] = df[\"claimReviewed\"].progress_apply(lambda x: map_minimal(x))\n",
    "np.random.seed(1)\n",
    "df = df.sample(frac=1).drop_duplicates(subset=\"claim_minimal\", keep=\"first\").sort_index().reset_index(drop = True)\n",
    "df[\"verdict\"] = df.verdict.map(lambda x: verdict_mapping[x] if x in verdict_mapping else \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Data/Cleaned_FactCheckData_nopreprocess_local.csv.gz\", compression=\"gzip\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
