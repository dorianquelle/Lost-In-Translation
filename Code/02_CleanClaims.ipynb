{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import os \n",
    "import re\n",
    "import httpcore\n",
    "setattr(httpcore, 'SyncHTTPTransport', any)\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import numpy as np\n",
    "translator = Translator()\n",
    "\n",
    "tqdm.pandas()\n",
    "df = pd.read_csv(\"../Data/Cleaned_FactCheckData_nopreprocess_local.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
    "# Tokenize the data\n",
    "df[\"tokens\"] = df[\"claimReviewed\"].progress_apply(lambda x: tokenizer(x).tokens())\n",
    "# All domains that have less than 100 fact-checks => Unknown/Other\n",
    "domains = df[\"domain\"].value_counts()\n",
    "domains = domains[domains < 100].index\n",
    "df.loc[df[\"domain\"].isin(domains), \"domain\"] = \"Unknown/Other\"\n",
    "df.loc[pd.isna(df.domain),\"domain\"] = \"Unknown/Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"claimReviewed\"] = df[\"tokens\"].progress_apply(lambda x: tokenizer.convert_tokens_to_string(x).replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\"))\n",
    "df[\"claimReviewed\"] = df[\"claimReviewed\"].progress_apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all 3-grams\n",
    "from nltk.util import ngrams as ngram_generator\n",
    "grams = {}\n",
    "\n",
    "for i,r in tqdm(df.iterrows(), total=len(df)):\n",
    "    tokens = r[\"tokens\"]\n",
    "    domain = r[\"domain\"]\n",
    "\n",
    "    ngrams = [list(ngram) for ngram in ngram_generator(tokens, 6)]\n",
    "    for ngram in ngrams:\n",
    "        ngram_string = tokenizer.convert_tokens_to_string(ngram)\n",
    "        # Add the ngram to the counter\n",
    "        if domain not in grams:\n",
    "            grams[domain] = {}\n",
    "\n",
    "        if ngram_string not in grams[domain]:\n",
    "            grams[domain][ngram_string] = 0\n",
    "        grams[domain][ngram_string] += 1\n",
    "\n",
    "# Sort each key by value\n",
    "domain_size = df.domain.value_counts().to_dict()\n",
    "\n",
    "for key in grams:\n",
    "    grams[key] = {k: v/domain_size[key] for k, v in sorted(grams[key].items(), key=lambda item: item[1], reverse=True)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of ngrams\n",
    "print(f\"Total number of ngrams: {sum([len(grams[key]) for key in grams])}\") #3,917,254\n",
    "\n",
    "# Print total number of ngrams with value > 0.01\n",
    "print(f\"Total number of ngrams with value > 0.01: {sum([len([k for k in grams[key] if grams[key][k] > 0.01]) for key in grams])}\") \n",
    "\n",
    "# Print total number of ngrams with value > 0.05\n",
    "print(f\"Total number of ngrams with value > 0.05: {sum([len([k for k in grams[key] if grams[key][k] > 0.05]) for key in grams])}\")\n",
    "\n",
    "# Print total number of ngrams with value > 0.10\n",
    "print(f\"Total number of ngrams with value > 0.10: {sum([len([k for k in grams[key] if grams[key][k] > 0.10]) for key in grams])}\")\n",
    "\n",
    "# Print total number of ngrams with value > 0.50\n",
    "print(f\"Total number of ngrams with value > 0.50: {sum([len([k for k in grams[key] if grams[key][k] > 0.50]) for key in grams])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all n-grams with value > 0.1 and make a dataframe\n",
    "ngrams = []\n",
    "for key in grams:\n",
    "    for ngram in grams[key]:\n",
    "        if grams[key][ngram] > 0.1:\n",
    "            ngrams.append([key, ngram, grams[key][ngram]])\n",
    "\n",
    "ngrams = pd.DataFrame(ngrams, columns=[\"domain\", \"ngram\", \"value\"])\n",
    "ngrams[\"remove\"] = False\n",
    "ngrams[\"remove_partial\"] = \"\"\n",
    "\n",
    "def translate(text):\n",
    "    try:\n",
    "        return translator.translate(text, dest=\"en\").text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# Translate the n-grams - to check whether they can be removed\n",
    "ngrams[\"translation\"] = ngrams[\"ngram\"].progress_apply(lambda x: translate(x))\n",
    "ngrams.to_csv(\"../Data/ngrams.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_set = set()\n",
    "remove_set.update([\"Fact Check:\",\"Fact-Check:\", \"FACT CHECK :\",\"Fact Check.\",\"[ Fakta atau Hoaks ]\"\n",
    "                   \"FactCheck:\", \"[SALAH]\", \"[No Ceará]\", \"【 錯 誤 】\",\"- Fakenews. pl\",\"- Fakenews.pl\"\n",
    "                   \"#Verificamos :\", \"# Verificamos:\",\"VERA FILES  \",\"இடம் - முபாரக்பூர்\",\": # Real\", \"# Real\",\"#Real\"\n",
    "                   \"# Verificamos :\", \"#Verificamos:\",\n",
    "                   \"Fact Check :\", \"Fact-Check :\",\"[ SALAH ]\",\"SATIRE :\", \"FAKE NEWS :\",\"Satire :\", \"SATIRE:\",\"Satire:\",\n",
    "                   \"É # FAKE\",\"Fake News :\",\"網 傳 影 片\", \"網 傳 圖 片\", \"網 傳 影 片 稱\", \"網 傳 圖 片 搭 配 訊 息\",\n",
    "                   \"Фейк : \",\"Манипуляция :\", \"Фотофейк :\",\"फैक्ट चेक :\",\"Claim : \",\"[ SPAM ]\",\n",
    "                   \"Analysis |\",\"Fake Video : \",\"É falso que\",\"MUSIC - CHECK\",\"WHATSAPP - CHECK\",\n",
    "                   \"Veja o que é # FATO ou # FAKE nas \",\"Non, cette vidéo ne montre pas\",\n",
    "                   \"Non, cette photo ne montre pas\",\"[ Fakta atau Hoax ]\",\"| Agência Lupa\",\"- ELLINIKA HOAX\",\n",
    "                   \"- Vishvas News\",\"Fact Check | Misbar\",\"- Malumatfuruş\",\"| Doğruluk Pay\",\"Full Fact\",\"Fake :\",\n",
    "                   \"- Cek Fakta\",\"- Vera Files\",\"- teyit. org\",\"| مسبار\",\"- factly\",\"- FACTLY\",\"Fact - check :\",\"[ Fakta atau Hoaks ]\",\"Quick  \",\"Saryusz - \",\"| Misbar\",\n",
    "                   \"| فحص الحقائق\",\"Fact Check NI\",\"|მითების დეტექტორი\",\"FALSE :\",\"( COM VÍDEO )\",\"Fact Check NI\",\"|მითების დეტექტორი\",\"| Fact Check\",\"| فحص الحقائق\", \"NEWS \",\n",
    "                   \"Fact check\",\"Fact Check\", \"Video show\", \"هذه الصورة\", \"Es falso que\",\"فیکٹ چیک\", \"- Cekfakta Tempo. co\",\"CEKFAKTA\", \". CO\"])\n",
    "\n",
    "\n",
    "claims = []\n",
    "for i,r in df.iterrows():\n",
    "    claim_string = r[\"claimReviewed\"]\n",
    "    # Sort the ngrams by length\n",
    "    for ngram in sorted(remove_set, key=len, reverse=True):\n",
    "        # not CLS or SEP\n",
    "        if ngram in claim_string:\n",
    "            if \"[CLS]\" in ngram or \"[SEP]\" in ngram:\n",
    "                cleaned_ngram = ngram.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
    "                claim_string = claim_string.replace(cleaned_ngram, \"\")\n",
    "            else:\n",
    "                claim_string = claim_string.replace(ngram, \"\")\n",
    "    claims.append(claim_string)\n",
    "    \n",
    "df[\"claimReviewed\"] = claims\n",
    "del claims\n",
    "\n",
    "# Regex sub 【.*?】\n",
    "special_pattern = re.compile(r\"【.*?】\")\n",
    "df[\"claimReviewed\"] = df[\"claimReviewed\"].apply(lambda x: special_pattern.sub(\"\",x))\n",
    "del special_pattern\n",
    "\n",
    "# Strip incase the ngram was at the beginning or end of the string and induced superfluous spaces\n",
    "df[\"claimReviewed\"] = df[\"claimReviewed\"].apply(lambda x: x.strip())\n",
    "\n",
    "# If first letter is : remove (This is often the case when the ngram is something like Fact-check :!)\n",
    "df[\"claimReviewed\"] = df[\"claimReviewed\"].apply(lambda x: x[1:].strip() if len(x) > 0 and x[0] == \":\" else x)\n",
    "\n",
    "# If the entire claim is enclosed by quotation marks (\"\" or ''), remove them In general. If first letter is punctuation, and the last letter is the same punctuation, remove them\n",
    "quotation_marks = \"\"\"\"“'“”‘„‚«»„“‹›‘’“‘「 」『 』\"\"\" #thanks chatgpt\n",
    "punctuation = string.punctuation\n",
    "def clean_punt(x):\n",
    "    if len(x) == 0:\n",
    "        return x\n",
    "    if len(x) > 0 and x[0] == x[-1] and not x[0].isalnum():\n",
    "        return x[1:-1]\n",
    "    elif x[0] in quotation_marks and x[-1] in quotation_marks:\n",
    "        return x[1:-1]\n",
    "    elif x[0] in quotation_marks and x[-1] not in quotation_marks:\n",
    "        # check whether every character after the last occurence of a quotation mark is punctuation\n",
    "        if all([c in punctuation + quotation_marks for c in x[x.rfind(x[0])+1:]]):\n",
    "            # In that case we remove the first quotation and the last quotation mark\n",
    "            return x[1:x.rfind(x[0])]\n",
    "        else:\n",
    "            return x\n",
    "    return x\n",
    "\n",
    "df[\"claimReviewed\"] = df[\"claimReviewed\"].apply(clean_punt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing the ngrams, we repeat the process of mapping to minimal\n",
    "def map_minimal(s):\n",
    "    # Remove non-alphanumeric characters except spaces using regex\n",
    "    result = re.sub(r'[^\\w]|[\\s\\n]', '', s)\n",
    "    # Remove spaces and convert the remaining characters to lowercase\n",
    "    return result.lower().replace(\" \", \"\")\n",
    "\n",
    "df[\"claimReviewed_mini\"] = df[\"claimReviewed\"].apply(map_minimal)\n",
    "print(f\"Number of claims after removing ngrams: {df.shape[0]} We are removing {df.claimReviewed_mini.duplicated().sum()} claims\")\n",
    "\n",
    "# Set Seed\n",
    "np.random.seed(1)\n",
    "df = df.sample(frac=1).drop_duplicates(subset=\"claimReviewed_mini\", keep=\"first\").sort_index().reset_index(drop = True)\n",
    "# Drop the minimal column\n",
    "df = df.drop(\"claimReviewed_mini\", axis=1)\n",
    "\n",
    "# remove claims that are empty\n",
    "df = df[df[\"claimReviewed\"] != \"\"].reset_index(drop=True)\n",
    "# remove claims that are NA\n",
    "df = df[df[\"claimReviewed\"].notna()].reset_index(drop=True)\n",
    "# remove claims that are of length < 10\n",
    "df = df[df[\"claimReviewed\"].apply(lambda x: len(x) > 10)].reset_index(drop=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks - Shape, Any NAs, Any Empties, Any len < 5\n",
    "print(df.shape)\n",
    "assert df[\"claimReviewed\"].isna().sum() == 0, \"NAs\"\n",
    "assert sum(df[\"claimReviewed\"] == \"\") == 0, \"Empties\"\n",
    "assert sum(df[\"claimReviewed\"].apply(lambda x: len(x) < 5)) == 0, \"Length < 5\"\n",
    "\n",
    "# Save the data\n",
    "df.to_csv(\"../Data/minimal_FactCheckData_local.csv.gz\", index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
