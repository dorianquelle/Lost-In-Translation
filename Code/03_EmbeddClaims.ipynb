{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import annoy\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "from bisect import bisect_left\n",
    "import datetime\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '9' \n",
    "model = sentence_transformers.SentenceTransformer('LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data and Run Sanity Checks\n",
    "df = pd.read_csv(\"../Data/minimal_FactCheckData_local.csv.gz\",compression = \"gzip\")\n",
    "\n",
    "df[\"datePublished\"] = pd.to_datetime(df.datePublished, errors = \"coerce\")\n",
    "df = df.dropna(subset = [\"datePublished\"])\n",
    "assert df[\"claimReviewed\"].isna().sum() == 0, \"NAs\"\n",
    "assert sum(df[\"claimReviewed\"] == \"\") == 0, \"Empties\"\n",
    "assert sum(df[\"claimReviewed\"].apply(lambda x: len(x) < 5)) == 0, \"Length < 5\"\n",
    "\n",
    "docs = df[\"claimReviewed\"].tolist()\n",
    "ids = df[\"claim_minimal\"].tolist()\n",
    "\n",
    "# Chunk docs into batches of 100.000\n",
    "docs_chunks = [docs[x:x+10000] for x in range(0, len(docs), 10000)]\n",
    "ids_chunks = [ids[x:x+10000] for x in range(0, len(ids), 10000)]\n",
    "\n",
    "# Embedd each chunk, and export as dictionary with tweet_id as key\n",
    "for i in range(len(docs_chunks)):\n",
    "    # Test if chunk is already embedded:\n",
    "    if os.path.isfile(f\"../Data/embeddings/embeddings_{i+1}.npy\"):\n",
    "        continue\n",
    "    print(f\"Embedding chunk {i+1} of {len(docs_chunks)}\")\n",
    "    print(f\"Starting at {datetime.datetime.now()}\")\n",
    "    embeddings = model.encode(docs_chunks[i], show_progress_bar=True, batch_size=256)  \n",
    "    embeddings_dict = dict(zip(ids_chunks[i], embeddings))\n",
    "    np.save(f\"../Data/Embeddings/embeddings_{i+1}.npy\", embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"datePublished\"] = pd.to_datetime(df[\"datePublished\"])\n",
    "DATES = df.set_index(\"claim_minimal\").datePublished.to_dict()\n",
    "\n",
    "ORIGINAL_IDS = df.claim_minimal.to_list()\n",
    "DEFAULT_DIMENSION = 768\n",
    "DEFAULT_TREES = 50\n",
    "DEFAULT_NEIGHBORS = 50\n",
    "DEFAULT_THRESHOLD = np.sqrt(0.8)\n",
    "DEFAULT_INCREASE = 2\n",
    "\n",
    "def load_embeddings(file_path, ids_order = ORIGINAL_IDS):\n",
    "    \"\"\"Load embeddings from a numpy file.\"\"\"\n",
    "    try:\n",
    "        embeddings_dict = np.load(file_path, allow_pickle=True).item()\n",
    "        embeddings_ordered = {id: embeddings_dict[id] for id in ids_order if id in embeddings_dict}\n",
    "        return embeddings_ordered\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load embeddings from {file_path} with error {e}\")\n",
    "        return None\n",
    "    \n",
    "def create_annoy_index(embeddings_dict, dimension=DEFAULT_DIMENSION, trees=DEFAULT_TREES):\n",
    "    \"\"\"Create an Annoy index from embeddings.\"\"\"\n",
    "    index = AnnoyIndex(dimension, 'angular')\n",
    "\n",
    "    assert list(embeddings_dict.keys()) == ORIGINAL_IDS, \"The embeddings_dict does not contain the same ids as the original dataframe. Error in create_annoy_index.\"\n",
    "\n",
    "    for i, v in tqdm(enumerate(embeddings_dict.values()), total = len(embeddings_dict)):\n",
    "        index.add_item(i, v)\n",
    "\n",
    "    index.build(trees)\n",
    "    return index\n",
    "\n",
    "def load_all_embeddings(folder_path):\n",
    "    \"\"\"Load all embeddings from a directory.\"\"\"\n",
    "    embeddings_dict = {}\n",
    "    # Get the list of filenames and sort them by the number included in the filename\n",
    "    filesnames = os.listdir(folder_path)\n",
    "    filesnames = [x for x in filesnames if x.endswith('.npy')]\n",
    "    filenames = sorted(filesnames, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "    \n",
    "    for file_name in filenames:\n",
    "        if file_name.endswith('.npy') and file_name.startswith('embeddings'):\n",
    "            embeddings_dict.update(load_embeddings(os.path.join(folder_path, file_name)))\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "def logging(string, colour, logging_lvl = 0):\n",
    "    \"\"\"Prints and logs a string with a timestamp.\"\"\"\n",
    "    now = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    if colour == \"red\":\n",
    "        print(f\"\\033[91m{string}Starting at {now}\\033[0m\")\n",
    "    elif colour == \"green\":\n",
    "        print(f\"\\033[92m{string}Starting at {now}\\033[0m\")\n",
    "    \n",
    "    if logging_lvl > 1:\n",
    "        # Write to log file\n",
    "        with open(\"log.txt\", \"a\") as f:\n",
    "            f.write(f\"{string}. Starting at {now}\\n\")\n",
    "\n",
    "def get_edge_list(embeddings_dict, index, start_neighbors=10, increase_rate=2, threshold=np.sqrt(0.8)):\n",
    "    \"\"\"Get a list of nearest neighbors for each item in the embeddings.\"\"\"\n",
    "    ids = list(embeddings_dict.keys())\n",
    "    assert ids == ORIGINAL_IDS, \"The embeddings_dict does not contain the same ids as the original dataframe. Error in get_edge_list.\"\n",
    "    edge_list = {}\n",
    "\n",
    "    for i, ind in tqdm(enumerate(ids), total = len(ids)):\n",
    "        neighbors = start_neighbors\n",
    "        nn, distances = index.get_nns_by_item(i, neighbors, include_distances=True)\n",
    "        \n",
    "        while distances and distances[-1] < threshold:\n",
    "            neighbors = int(neighbors * increase_rate)\n",
    "            nn, distances = index.get_nns_by_item(i, neighbors, include_distances=True)\n",
    "        \n",
    "        insertion_point = bisect_left(distances, threshold)\n",
    "        edge_list[ind] = {ids[nn[x]]: (2-(distances[x]**2))/2 for x in range(insertion_point) if DATES[ids[nn[x]]] < DATES[ind]} # Exactly the wrong way around. \n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging(\"\",\"green\")\n",
    "# Get all embeddings from the folder\n",
    "logging(\"Loading Embeddings. \", colour = \"green\")\n",
    "embeddings = load_all_embeddings(folder_path = \"../Data/embeddings/\")\n",
    "# Create the Annoy index\n",
    "logging(\"Creating Annoy index. \", colour = \"green\")\n",
    "annoy_index = create_annoy_index(embeddings_dict = embeddings, dimension=DEFAULT_DIMENSION, trees=DEFAULT_TREES)\n",
    "# Create the edge_list\n",
    "logging(\"Creating Edge List. \", colour = \"green\")\n",
    "edge_list = get_edge_list(embeddings_dict = embeddings,\n",
    "                          index = annoy_index,\n",
    "                          start_neighbors=DEFAULT_NEIGHBORS,\n",
    "                          increase_rate=DEFAULT_INCREASE,\n",
    "                          threshold=DEFAULT_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Edge List\n",
    "import pickle\n",
    "logging(\"Pickling Edge List. \", colour = \"green\")\n",
    "with open(\"../Data/edge_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(edge_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
